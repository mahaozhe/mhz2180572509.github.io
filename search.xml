<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>A Brief Tutorial to Use PySC2 Learning Environment (Overview)</title>
    <url>/2022/1-tutorial-pysc2-1/</url>
    <content><![CDATA[<p><code>PySC2</code> is DeepMind's Python component of the StarCraft II
Learning Environment (SC2LE). This is a brief tutorial about how to use
the interface as an interactable environment. You can also refer to the
<a href="https://github.com/mhz2180572509/HowToUsePySC2/">repo on
GitHub</a>.</p>
<span id="more"></span>
<h1 id="installation">Installation</h1>
<p><strong>It's strongly recommended to use virtual environments to
manage packages.</strong></p>
<p>You may need to run <code>pip install --upgrade pip</code> before all
the following steps.</p>
<h2 id="install-on-windows">Install on Windows</h2>
<ul>
<li><p>Get <code>PySC2</code></p>
<p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install pysc2</span><br></pre></td></tr></table></figure> <strong>Not recommended unless you need:</strong>
download from source, please refer to: <a
href="https://github.com/deepmind/pysc2#from-source"
class="uri">https://github.com/deepmind/pysc2#from-source</a></p></li>
<li><p>Get <em>StarCraft II</em> game</p>
<ul>
<li>Download from Blizzard official network: <a
href="https://starcraft2.com/"
class="uri">https://starcraft2.com/</a>.</li>
<li>You can firstly install the Battle.net Agent/Desktop and install the
StarCraft II game in the game launcher: <a
href="https://www.blizzard.com/en-us/apps/battle.net/desktop"
class="uri">https://www.blizzard.com/en-us/apps/battle.net/desktop</a></li>
<li>If you customized the install-location, you might need to set the
<code>SC2PATH</code> environment variable with your new location.</li>
</ul></li>
<li><p>Get maps</p>
<p>Create a folder named <code>Maps</code> in the game root folder. Then
download the maps and mini-maps and extract them into <code>Maps</code>.
You can start from the mini-games. For example, after you extract the
maps in the folder, the path should be like this:
<code>GameFolder/Maps/Melee/XXX.SC2Map</code>.</p>
<ul>
<li>All map packs: <a
href="https://github.com/Blizzard/s2client-proto#downloads"
class="uri">https://github.com/Blizzard/s2client-proto#downloads</a></li>
<li>Mini-games: <a
href="https://github.com/deepmind/pysc2/releases/download/v1.2/mini_games.zip"
class="uri">https://github.com/deepmind/pysc2/releases/download/v1.2/mini_games.zip</a></li>
<li>Melee maps: <a
href="http://blzdistsc2-a.akamaihd.net/MapPacks/Melee.zip"
class="uri">http://blzdistsc2-a.akamaihd.net/MapPacks/Melee.zip</a></li>
</ul>
<p>The password to extract the packages is
<strong>iagreetotheeula</strong></p></li>
</ul>
<h2 id="install-on-linux">Install on Linux</h2>
<ul>
<li>Install <code>PySC2</code>: <code>pip install pysc2</code></li>
<li>Install StarCraft II game:
<ul>
<li>Download the linux package: <a
href="https://github.com/Blizzard/s2client-proto/blob/master/README.md#linux-packages"
class="uri">https://github.com/Blizzard/s2client-proto/blob/master/README.md#linux-packages</a>
<ul>
<li>Can using the command: <code>wget
http://blzdistsc2-a.akamaihd.net/Linux/SC2.4.10.zip</code></li>
</ul></li>
<li>Unzip the file at the home path: <code>unzip -o SC.X.X.zip</code>
<ul>
<li>The password is <strong>iagreetotheeula</strong></li>
</ul></li>
<li>It seems that the maps are already included in the
<code>[StarCraftII]/Maps/</code> folder, if you need to install the
replays, you need to download and unzip them into the
<code>[StarCraftII]/Replays/</code> folder manually.</li>
</ul></li>
</ul>
<h1 id="run-some-built-in-examples">Run Some Built-in Examples</h1>
<ul>
<li>Run the built-in agent example: <code>python -m pysc2.bin.agent
--map Simple64</code></li>
<li>Run your own agent: <code>python -m pysc2.bin.agent --map
CollectMineralShards --agent
pysc2.agents.scripted_agent.CollectMineralShards</code>
<ul>
<li>Will have more detailed instructions to write and run our
self-defined agents later.</li>
</ul></li>
<li>Run two agents against each other <code>python -m pysc2.bin.agent
--map Simple64 --agent2
pysc2.agents.random_agent.RandomAgent</code></li>
<li>Play the games as a human: <code>python -m pysc2.bin.play --map
Simple64</code></li>
<li>List the maps: <code>python -m pysc2.bin.map_list</code></li>
<li>Watch the replay: <code>python -m pysc2.bin.play --replay
&lt;path-to-replay&gt;</code>
<ul>
<li>Running an agent and playing as a human save a replay by default.
You can watch that replay by running the above command.</li>
</ul></li>
</ul>
<h1 id="official-doc-of-pysc2-and-useful-links">Official Doc of PySC2
and Useful Links</h1>
<ul>
<li>Before we start, it's very <strong>important</strong> to read the
official document from <code>PySC2</code>: <a
href="https://github.com/deepmind/pysc2/blob/master/docs/environment.md"
class="uri">https://github.com/deepmind/pysc2/blob/master/docs/environment.md</a>
<ul>
<li>You can briefly know about (but you may still be very confused
with): the structure of <code>pysc2</code>, the observations and actions
provided by <code>pysc2</code>, the environment for reinforcement
learning, and so on.</li>
</ul></li>
<li>Introductions to the mini-games (read it when you need): <a
href="https://github.com/deepmind/pysc2/blob/master/docs/mini_games.md"
class="uri">https://github.com/deepmind/pysc2/blob/master/docs/mini_games.md</a></li>
</ul>
<h1 id="start-to-use-pysc2">Start to Use PySC2</h1>
<h2 id="approach-1">Approach 1</h2>
<p>In thie approach, you can only customize the agent, and run the agent
by running the above commands.</p>
<ul>
<li>In this approach, your agent gets observation from the environment
and returns actions to interact with it.</li>
<li>You need to run the agent using the built-in module
<code>pysc2.bib.agent</code></li>
<li>It's hard to realize a learning agent.</li>
<li>It's a good way to start to know about the environment.</li>
<li>Detailed instructions at <a href="/2022/2-tutorial-pysc2-2/" title="Use PySC2 Learning Environment - Define Your Own Agent">Use PySC2 Learning Environment - Define Your Own Agent</a> and <a
href="https://github.com/mhz2180572509/HowToUsePySC2/blob/main/Approach_1.py">example
codes</a>.</li>
</ul>
<h2 id="approach-2">Approach 2</h2>
<p>In thie approach, you can customize both the environment and the
agent, run your codes independently without any built-in modules. * More
flexible. * Can define our logics in your agent, like learning and doing
inference. * Can run the codes directly without any built-in modules. *
Can debug easily. * Detailed instructions at <a href="/2022/3-tutorial-pysc2-3/" title="Use PySC2 Learning Environment - Control Your Own Environment and Agent">Use PySC2 Learning Environment - Control Your Own Environment and Agent</a> and <a
href="https://github.com/mhz2180572509/HowToUsePySC2/blob/main/Approach_2.py">example
codes</a>.</p>
<h1 id="main-reference">Main Reference</h1>
<ul>
<li>PySC2 introduction and documents by DeepMind: <a
href="https://github.com/deepmind/pysc2"
class="uri">https://github.com/deepmind/pysc2</a></li>
</ul>
]]></content>
      <categories>
        <category>Tutorial</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>PySC2</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title>[DRQN] Deep Recurrent Q-Learning for Partially Observable MDPs</title>
    <url>/2022/10-RLPaper-MF-2-DRQN/</url>
    <content><![CDATA[<blockquote>
<p>Matthew Hausknecht and Peter Stone. 2015. Deep Recurrent Q-Learning
for Partially Observable MDPs. In 2015 AAAI Fall Symposium Series.
Retrieved August 8, 2022 from
https://www.aaai.org/ocs/index.php/FSS/FSS15/paper/view/11673</p>
</blockquote>
<span id="more"></span>
<figure>
<img src="/images/RLPapers/Model-Free/DeepQLearning/2-1.png"
alt="Deep Recurrent Q-Learning for Partially Observable MDPs" />
<figcaption aria-hidden="true">Deep Recurrent Q-Learning for Partially
Observable MDPs</figcaption>
</figure>
<h1 id="overview">Overview</h1>
<p>The paper proposed the <strong><em>Deep Recurrent
Q-Networks</em></strong> (<strong><em>DRQN</em></strong>) algorithm
based on the famous DQN algorithm. One main standpoint the authors
argued in the paper is that, for the Atari games, only one frame of the
game screen is the <strong>partial observation</strong> of the
environment. So the paper proposed to introduce the Long Short Term
Memory (LSTM) block to the Q-network to catch the information in the
sequence, in which case, only one frame of the screen can be used to
input to the network and replicates the performance of DQN. The paper
conducts experiments on a modified <em>flickering Pong</em> game and
other Atari games.</p>
<h1 id="main-problems-to-solve">Main Problems to Solve</h1>
<p>For the original DQN, the input is the stack of four continuous
frames to include the information like the velocity and acceleration of
the objects. Any game that requires a memory of more than four frames
will appear <strong>non-Markov</strong>. What’s more , for most
real-world tasks, the agent can only observe it partially, which could
be modeled by <strong><em>Partially Observed Markov Decision
Process</em></strong> (<strong><em>POMDP</em></strong>).</p>
<h1 id="main-innovations">Main Innovations</h1>
<p>The main modification of DRQN from DQN are:</p>
<ul>
<li>Replace the first post-convolutional fully-connected layer in DQN
with a <strong>recurrent LSTM</strong> block.</li>
<li>For the input of the model, it no longer uses the stack of four
continuous frames from the game (which is implemented by DQN), but use
<strong>only one frame</strong> of the screen.</li>
</ul>
<p>To train the DRQN model, the paper proposed two main methods, and
show experiments only for the second one, as the paper indicates that
their performance are almost the same.</p>
<ul>
<li><strong><em>Bootstrapped Sequential Updates</em></strong>: Episodes
are selected randomly from the replay memory and updates begin at the
beginning of the episode and proceed forward through time to the
conclusion of the episode, which mean use the whole trajectory to train
the model, the states along the trajectory will be input into the hidden
units of LSTM module. This method violates the random sampling policy of
DQN but can converge more quickly.</li>
<li><strong><em>Bootstrapped Random Updates</em></strong>: Episodes are
selected randomly from the replay memory and updates begin at random
points in the episode and proceed for only unroll iterations timesteps.
This is the same as DQN's updating policy, the hidden units are
initialized as zero, which learns a bit slower than the above one.</li>
</ul>
<p>The main advantage of DRQN algorithm is that it can integrate
information across frames to detect relevant information despite seeing
only one single frame at each step. More importantly, the model trained
on POMDP can work normally on environments modeled by MDP.</p>
]]></content>
      <categories>
        <category>RL Paper Notes</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Paper Notes</tag>
        <tag>Model-Free RL</tag>
        <tag>Deep Q-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>[Dueling DQN] Dueling Network Architectures for Deep Reinforcement Learning</title>
    <url>/2022/12-RLPaper-MF-4-DuelingDQN/</url>
    <content><![CDATA[<blockquote>
<p>Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and
Nando Freitas. 2016. Dueling Network Architectures for Deep
Reinforcement Learning. In Proceedings of The 33rd International
Conference on Machine Learning, PMLR, 1995–2003. Retrieved August 8,
2022 from https://proceedings.mlr.press/v48/wangf16.html</p>
</blockquote>
<span id="more"></span>
<figure>
<img src="/images/RLPapers/Model-Free/DeepQLearning/4-1.png"
alt="Dueling Network Architectures for Deep Reinforcement Learning" />
<figcaption aria-hidden="true">Dueling Network Architectures for Deep
Reinforcement Learning</figcaption>
</figure>
<h1 id="overview">Overview</h1>
<p>The paper proposed a creative network <strong>architecture</strong>
called <strong><em>dueling network</em></strong> to <strong>decouple the
value and advantage</strong> in deep Q-networks algorithm. The
architecture can be used to replace the original network in any
applicable deep RL algorithm without imposing any change to the
underlying algorithm. The authors conducted experiments to show better
performances that outperform the state-of-the-art algorithms (2016-6) on
Atari games domain.</p>
<h1 id="main-problems-to-solve">Main Problems to Solve</h1>
<p>The new architecture main solves the following problems:</p>
<ul>
<li>In most environments, for many states, the selection of actions is
almost irrelevant, which means, for these states, selecting any action
by the agent does not affect the environment any more.</li>
<li>In most popular deep RL algorithms, like DQN, DDQN, the difference
of the Q-values among different actions are usually very small, which is
easy to be influenced by any noise or oscillation.</li>
</ul>
<h1 id="main-innovations">Main Innovations</h1>
<p>The proposed architecture is shown in the following figure:</p>
<figure>
<img src="/images/RLPapers/Model-Free/DeepQLearning/4-2.png"
alt="Dueling network architecture" />
<figcaption aria-hidden="true">Dueling network architecture</figcaption>
</figure>
<p>At the end of the convolutional layers, the dueling network separate
the output to two streams, one to estimate the state value <span
class="math inline">\(V(s)\)</span> (a scaler) and one to estimate the
<strong><em>advantage function</em></strong> of each action for the
current state <span class="math inline">\(A(s,a)\)</span> (a vector with
length of <span class="math inline">\(|A|\)</span>). Then the network
use a sepcial <strong>aggregating layer</strong> to combine the value
and the advantage together to output the Q-value <span
class="math inline">\(Q(s,a)\)</span>.</p>
<p>The relationship among these qualities under a specific policy <span
class="math inline">\(\pi\)</span> is:</p>
<p><span class="math display">\[\begin{equation}
A^{\pi}(s,a)=Q^{\pi}(s,a)−V^{\pi}(s)
\end{equation}\]</span></p>
<p>which indicates that the advantage is the Q-value but measured by the
value of that state, in this case, we actually have: <span
class="math inline">\(\mathbb{E}_{a \backsim
\pi(s)}[A^{\pi}(s,a)]=0\)</span>. The advantage is a <strong>relative
measure</strong> of the importance of each action.</p>
<p>One important problem is that: actually, in deep Q-networks based
algorithms, we only estimate the final output Q-values in the algorithm,
but not explicitly estimate or give out the state values and action
advantages, and we cannot <strong>recover</strong> these two values only
based on the given Q-value. So, to address this issue of identifiability
problem, the paper <strong>forced the advantage function estimator to
offset by the maximal of all advantages or the average of all
advantages</strong>:</p>
<ul>
<li><span
class="math inline">\(Q(s,a)=V(s)+(A(s,a)−\max_{a&#39;}⁡{A(s,a&#39;)})\)</span></li>
<li><span
class="math inline">\(Q(s,a)=V(s)+(A(s,a)−\frac{1}{|A|}\sum_{a&#39;}{A(s,a&#39;)})\)</span></li>
</ul>
<p>This trick, on the one hand, loses the original semantics of V and A
because they are now off-target by a constant, but on the other hand, it
increases the stability of the optimization. What's more, this doesn't
change the relative rank of the A (and hence Q) values and the values
can be computed <strong>automatically</strong>.</p>
<p>The paper evaluated the performance of the above two methods, and
show the results based on the second equation.</p>
]]></content>
      <categories>
        <category>RL Paper Notes</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Paper Notes</tag>
        <tag>Model-Free RL</tag>
        <tag>Deep Q-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>[A3C] Asynchronous Methods for Deep Reinforcement Learning</title>
    <url>/2022/15-RLPaper-MF-PG-1-A3C/</url>
    <content><![CDATA[<blockquote>
<p>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves,
Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
2016. Asynchronous Methods for Deep Reinforcement Learning. In
Proceedings of The 33rd International Conference on Machine Learning,
PMLR, 1928–1937. Retrieved October 4, 2022 from
https://proceedings.mlr.press/v48/mniha16.html</p>
</blockquote>
<span id="more"></span>
<figure>
<img src="/images/RLPapers/Model-Free/PolicyGradients/1-1.png"
alt="Asynchronous Methods for Deep Reinforcement Learning" />
<figcaption aria-hidden="true">Asynchronous Methods for Deep
Reinforcement Learning</figcaption>
</figure>
<h2 id="overview">Overview</h2>
<p>The paper proposed a general framework to train RL agents
asynchronously. The main idea is to initialize multiple learners on
multiple threads of the multiple-core CPUs. There is a
<strong>global</strong> learner and multiple <strong>actor</strong>
learners. The actor learners push their parameters to the global learner
and pull parameters from it. In this case, the samples from multiple
learners can <strong>de-correlate</strong> the relations from the
sequential trajectories, so that on-line learning can be conducted in
deep RL domain, and save time and computing resources effectively. The
paper implemented the asynchronous approach into four existing RL
algorithms:</p>
<ul>
<li>Asynchronous one-step Q-learning</li>
<li>Asynchronous one-step SARSA</li>
<li>Asynchronous n-step Q-learning</li>
<li><strong><em>Asynchronous Advantage Actor-Critic
(A3C)</em></strong></li>
</ul>
<p>where the last one achieves best performance. The paper conducted
some experiments on Atari games and show the A3C algorithm outperforms
the best current baselines.</p>
<h2 id="main-problems-to-solve">Main Problems to Solve</h2>
<ol type="1">
<li>Many deep reinforcement learning algorithms turns to off-policy
approaches, as online learning cannot decorrelate the relations among
the sequential transitions. So many techniques are proposed, e.g.,
experience replay to <strong>aggregate over memory</strong>. In this
case, they usually requires off-policy learning.</li>
<li>The experience replay based algorithms or massively distributed
architectures use more memory and computation per real interaction.</li>
</ol>
<h2 id="main-innovations">Main Innovations</h2>
<p>The paper proposed a conceptually simple and lightweight framework
for deep reinforcement learning that uses asynchronous gradient descent
for optimization of deep neural network controllers. The algorithm
instantiates multiple learners on multiple threads on multiple-core CPUs
and one global learner. The workers parallelly learn in their own
environment instances and share gradients or parameters with the global
learner. In this case, there are several advantages:</p>
<ol type="1">
<li>The workers sample and learn by interacting with their own
environments, so the samples are not correlated with each other, which
solves the main problem of on-line RL approaches.</li>
<li>Since the parallelism can decorrelates the data, then no experience
replay is needed, which saves much time and computing resources, makes
it possible to train agents on multiple-core CPUs but not GPUs, and even
use much shorter time than training on GPUs.</li>
<li>The main idea can make on-line algorithms to be implemented into
deep RL domains, like actor-critic.</li>
<li>Compared with the massively distributed approaches, the workers and
the global learner are set on one single machine but multiple cores of
CPU, so there is no need to cost time and resources on the communication
between the machines.</li>
<li>Different learners can have different exploration policies, so that
better de-correlate the samples.</li>
</ol>
]]></content>
      <categories>
        <category>RL Paper Notes</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Paper Notes</tag>
        <tag>Model-Free RL</tag>
        <tag>Policy Gradients</tag>
      </tags>
  </entry>
  <entry>
    <title>[DDQN] Deep Reinforcement Learning with Double Q-Learning</title>
    <url>/2022/11-RLPaper-MF-3-DDQN/</url>
    <content><![CDATA[<blockquote>
<p>Hado van Hasselt, Arthur Guez, and David Silver. 2016. Deep
Reinforcement Learning with Double Q-Learning. Proceedings of the AAAI
Conference on Artificial Intelligence 30, 1 (March 2016). DOI:
https://doi.org/10.1609/aaai.v30i1.10295</p>
</blockquote>
<span id="more"></span>
<figure>
<img src="/images/RLPapers/Model-Free/DeepQLearning/3-1.png"
alt="Deep Reinforcement Learning with Double Q-Learning" />
<figcaption aria-hidden="true">Deep Reinforcement Learning with Double
Q-Learning</figcaption>
</figure>
<h1 id="overview">Overview</h1>
<p>The paper mainly answered the questions that why there exists
<strong>overestimation</strong> in the previous reinforcement learning
(RL) algorithms and how serious the overestimation is to harm the
performance and lead to sub-optimal policies
<strong>theoretically</strong>. The paper gives substantial proof of
this problem and introduced the idea of <strong><em>Double
Q-Learning</em></strong> [1] to the function approximation scenario to
propose the <strong><em>Double Deep Q-Networks</em></strong>
(<strong><em>DDQN</em></strong>) algorithm. The authors show
experimental results on Atari games to compare the DDQN with the
standard DQN and show it can not only yields more accurate value
estimates, but also leads to much higher scores (better policies).</p>
<h1 id="main-problems-to-solve">Main Problems to Solve</h1>
<p>The paper mainly want to overcome/relieve the overestimation problem
in standard DQN algorithm.</p>
<h1 id="main-innovations">Main Innovations</h1>
<p>The core idea for DDQN algorithm to reduce overestimation is to
<strong>decompose the max operation in the TD-target into <em>action
selection</em> and <em>action evaluation</em></strong>. DDQN is based on
the standard DQN algorithm and uses the existing architecture
<strong>without requiring additional networks or
parameters</strong>.</p>
<p>In the DQN algorithm, it initialized two networks, one is called
<strong>(online) policy network</strong> (with parameters <span
class="math inline">\(\theta\)</span>) and another is called
<strong>target network</strong> (with parameters <span
class="math inline">\(\theta&#39;\)</span>). The parameters are copied
from policy network to target network periodically. To update parameters
of the policy network at each step, the algorithm randomly draw
transition samples from the experience replay buffer and minimize the
distance between the <strong>TD-target</strong> <span
class="math inline">\(Y\)</span> and the current Q-value.</p>
<p>The main difference is how to compute the TD-target:</p>
<ul>
<li><p>For the standard DQN:</p>
<p><span class="math display">\[\begin{equation}
  Y^{DQN}=r_{t+1}+\gamma \max_{a&#39;}⁡{Q(s_{t+1},a&#39;|\theta&#39;)}
  \end{equation}\]</span></p>
<p>which can be written as:</p>
<p><span class="math display">\[\begin{equation}
  Y^{DQN}=r_{t+1}+\gamma
Q(s_{t+1},\arg\max_{a&#39;}{Q(s_{t+1},a&#39;|\theta&#39;)}|\theta&#39;)
  \end{equation}\]</span></p>
<p>where we can see that the selection of the action with maximal
Q-value and compute the corresponding Q-value is all conducted by the
target network (so it directly return the maximal Q-value among the
outputs from the target network).</p></li>
<li><p>For the double DQN:</p>
<p><span class="math display">\[\begin{equation}
  Y^{DDQN}=r_{t+1}+\gamma
Q(s_{t+1},\arg\max_{a&#39;}{Q(s_{t+1},a&#39;|\theta)}|\theta&#39;)
  \end{equation}\]</span></p>
<p>where we can see that the selection of the action with maximal
Q-value is conducted by the policy network but the computation of the
corresponding Q-value is conducted by the target network.</p></li>
</ul>
<p>The process to compute the TD-target can be described as follows:</p>
<ol type="1">
<li>Input the state <span class="math inline">\(s_{t+1}\)</span> into
the policy network (with parameters <span
class="math inline">\(\theta\)</span>), select the action with the
maximal Q-value from the output head, suppose the action is <span
class="math inline">\(a^+\)</span>.</li>
<li>Input the state <span class="math inline">\(s_{t+1}\)</span> into
the target network (with parameters <span
class="math inline">\(\theta&#39;\)</span>), return the Q-value of <span
class="math inline">\(Q(s_{t+1},a^+)\)</span> from the output head.</li>
<li>Use the returned Q-value to compute the TD-target.</li>
</ol>
<h1 id="reference">Reference</h1>
<p>[1] Hado Hasselt. 2010. Double Q-learning. In Advances in Neural
Information Processing Systems, Curran Associates, Inc. Retrieved August
8, 2022 from
https://proceedings.neurips.cc/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html</p>
]]></content>
      <categories>
        <category>RL Paper Notes</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Paper Notes</tag>
        <tag>Model-Free RL</tag>
        <tag>Deep Q-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>[PER] Prioritized Experience Replay</title>
    <url>/2022/13-RLPaper-Te-ER-1-PER/</url>
    <content><![CDATA[<blockquote>
<p>Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. 2016.
Prioritized Experience Replay.
DOI:https://doi.org/10.48550/arXiv.1511.05952</p>
</blockquote>
<span id="more"></span>
<figure>
<img src="/images/RLPapers/Techniques/Experience-Replay/Images/1-1.png"
alt="Prioritized Experience Replay" />
<figcaption aria-hidden="true">Prioritized Experience
Replay</figcaption>
</figure>
<h1 id="overview">Overview</h1>
<p>The paper proposed a <strong><em>prioritized experience
replay</em></strong> (<strong><em>PER</em></strong>) framework to be
implemented in to experience-replay-based RL algorithms to replace the
original uniformly and randomly draw samples. The paper further proposed
two methods to solve problems that introduced by the prioritized: one is
the diversity of loss, another one is the introduced bias. The paper
evaluated the framework integrated into the DDQN algorithm, and show
better performances than the state-of-the-art algorithms (2016).</p>
<h1 id="main-problems-to-solve">Main Problems to Solve</h1>
<p>The prioritized experience replay method is proposed for more
efficiently and more effectively learning of the experience-replay-based
deep RL algorithms. As in the previous experience replay method, all
samples to train the model are drawn uniformly from the replay buffer at
random, however, many transitions are not useful to update or to
optimize the model in the desired direction.</p>
<h1 id="main-innovations">Main Innovations</h1>
<p>The main idea for the framework is to sample transitions from the
replay buffer using the magnitude of the TD-error <span
class="math inline">\(\delta\)</span> as the priority. Intuitively, the
larger of the TD-error for one transition, the distance between the
TD-target and the currently estimated Q-value will be larger, then the
agent should learn more from it than others.</p>
<p>However, there are two main problems that caused by the prioritized
experience replay: the <strong>loss diversity decreasing</strong> and
the <strong>bias introduced</strong>.</p>
<p>Firstly, the loss diversity will decrease. As the samples are drawn
based on the TD-error at the first computation when storing them into
the buffer, so some samples with very low TD-errors will have very low
probabilities to be selected or even never be selected, in which case,
the framework will focus on a very small subset of the experiences,
which leads the system to over-fitting.</p>
<p>To solve this problem, the paper proposed <strong><em>stochastic
prioritization</em></strong>, that interpolates between pure greedy
prioritization and uniform random sampling. The probability to select
one sample <span class="math inline">\(i\)</span> is defined as:</p>
<p><span class="math display">\[\begin{equation}
P(i)=\frac{p_i^{\alpha}}{\sum_k{p_k^{\alpha}}}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> determines how much
prioritization is used, with <span
class="math inline">\(\alpha=0\)</span> corresponding to the uniform
case.</p>
<p>Secondly, the prioritized experience replay introduced bias, as the
target/real distribution changes when we draw samples non-uniformly. To
solve this problem, the paper proposed to correct the bias by using the
<strong><em>importance-sampling weights</em></strong> which is defined
as:</p>
<p><span class="math display">\[\begin{equation}
w_i=\left(\frac{1}{N} \times \frac{1}{P(i)}\right)^{\beta}
\end{equation}\]</span></p>
<p>which can be folded into the Q-learning update by using <span
class="math inline">\(w_i \delta_i\)</span> instead of <span
class="math inline">\(\delta_i\)</span>. The hyper-parameter <span
class="math inline">\(\beta\)</span> controls the degree to compensate
the bias. The paper proposed to use an
<strong><em>annealing</em></strong> method to set the <span
class="math inline">\(\beta\)</span> as a very small value at the
beginning and define a schedule to make it reach 1 at the end of the
training.</p>
]]></content>
      <categories>
        <category>RL Paper Notes</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Paper Notes</tag>
        <tag>Experience Replay</tag>
        <tag>RL Techniques</tag>
      </tags>
  </entry>
  <entry>
    <title>[NoisyNet] Noisy Networks for Exploration</title>
    <url>/2022/14-RLPaper-Te-Ex-1-NoisyNet/</url>
    <content><![CDATA[<blockquote>
<p>Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick,
Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier
Pietquin, Charles Blundell, and Shane Legg. 2019. Noisy Networks for
Exploration. DOI: https://doi.org/10.48550/arXiv.1706.10295</p>
</blockquote>
<span id="more"></span>
<figure>
<img src="/images/RLPapers/Techniques/Exploration/Images/1-1.png"
alt="Noisy Networks for Exploration" />
<figcaption aria-hidden="true">Noisy Networks for
Exploration</figcaption>
</figure>
<h1 id="overview">Overview</h1>
<p>The paper proposed a general framework
<strong><em>NoisyNet</em></strong> to introduce <strong>parametric
noise</strong> to add to the weights in the network and aid the agent to
exploration more efficiently and effectively. The framework can be
implemented into any RL algorithms with <strong>gradient
descent</strong>, like DQN, DDQN, Dueling DQN, DDPG, A3C, TRPO and so
on, which also add little computational overhead. The <em>NoisyNet</em>
can replace the original exploration policies, like <span
class="math inline">\(\epsilon\)</span>-greedy exploration. The paper
conducted experiments to compare the DQN, Dueling DQN and A3C algorihtms
with <em>NoisyNet</em> respectively with the original version of these
algorithms, the results show higher scores for a wide range of Atari
games.</p>
<h1 id="main-problems-to-solve">Main Problems to Solve</h1>
<p>For traditional exploration policies, like entropy reward and <span
class="math inline">\(\epsilon\)</span>-greddy, there needs additional
hyper-parameters and more importantly, the policy is independent of the
states and policies learned by the agents.</p>
<h1 id="main-innovations">Main Innovations</h1>
<p>The key idea of the <em>NoisyNet</em> is to add a parametric noise to
the parameters (weights) (to be learned) of the networks. The added
noise can help to make the agent to select creative behaviors. More
importantly, as the noise is parametric, and the parameters are also
updated in the reinforcement learning algorithm by gradient descent with
the original parameters, so the noise can be controlled by the model
during the learning procedure and therefore, the noise is dependent of
the states and the policy.</p>
<p>The main method to change an original parameter <span
class="math inline">\(\theta\)</span> to a new set of learnable
parameters is to define:</p>
<p><span class="math display">\[\begin{equation}
\theta^{new}=\mu^{\theta}+\Sigma^{\theta} \odot \epsilon^{\theta}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> can be seen as the
original weight, and <span class="math inline">\(\Sigma\)</span> is the
factor to be multiplied to the noise to control the noise to be added
onto the original weight, <span class="math inline">\(\epsilon\)</span>
is the generated <strong>zero-mean noise with fixed statics</strong>,
<strong>element-wise multiplied</strong> by the control factor. In this
case, the original parameter <span class="math inline">\(\theta\)</span>
to learn is replaced by a set of parameter <span
class="math inline">\(\zeta=[\mu^{\theta},\Sigma^{\theta}]\)</span>. We
can see from the equation, the <em>NoisyNet</em> doesn't change or
control the distribution of the parameters in the network, but only
learn to control the intensity of the noise
<strong>automatically</strong>. The parameters of the noise <span
class="math inline">\(\Sigma\)</span> can also be learned in the RL
algorithm accompanying the parameters from the model.</p>
<p>The paper proposed two approaches to generate noise:</p>
<ul>
<li><strong>Independent Gaussian noise</strong>: uses an independent
Gaussian noise entry per weight.</li>
<li><strong>Factorised Gaussian noise</strong>: uses an independent
noise per each output and another independent noise per each input.</li>
</ul>
<h1 id="implementation-details">Implementation Details</h1>
<p>The paper implements the <em>NoisyNet</em> framework into three RL
algorithms to build the Noisy-DQN, Noisy-Dueling-DQN, and the Noisy-A3C
and compare them with the original algorithms respectively.</p>
<ol type="1">
<li>For the noisy-based algorithms, the original exploration policies,
like <span class="math inline">\(\epsilon\)</span>-greedy is
canceled.</li>
<li>A noisy network agent samples a new set of parameters after every
step of optimization. Between optimization steps, the agent acts
according to a fixed set of parameters.</li>
<li>The <em>NoisyNet</em> is not implemented into all parameters in the
original network, but only replace parts of them, like in the DQN and
Dueling DQN, the noise is only added onto parameters in the fully
connected layers.</li>
</ol>
]]></content>
      <categories>
        <category>RL Paper Notes</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Paper Notes</tag>
        <tag>RL Techniques</tag>
        <tag>Exploration</tag>
      </tags>
  </entry>
  <entry>
    <title>[TRPO] Trust Region Policy Optimization</title>
    <url>/2022/16-RLPaper-MF-PG-2-TRPO/</url>
    <content><![CDATA[<blockquote>
<p>John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and
Philipp Moritz. 2015. Trust Region Policy Optimization. In Proceedings
of the 32nd International Conference on Machine Learning, PMLR,
1889–1897. Retrieved October 4, 2022 from
https://proceedings.mlr.press/v37/schulman15.html</p>
</blockquote>
<span id="more"></span>
<figure>
<img src="/images/RLPapers/Model-Free/PolicyGradients/2-1.png"
alt="Trust Region Policy Optimization" />
<figcaption aria-hidden="true">Trust Region Policy
Optimization</figcaption>
</figure>
<h1 id="overview">Overview</h1>
<p>The paper proposed a new <strong>practical</strong> algorithm called
<strong><em>Trust Region Policy Optimization</em></strong>
(<strong><em>TRPO</em></strong>). The algorithm is
<strong>practical</strong> as it can be implemented into plenty
large-space environments and continuous control problems. The main
innovation of the TPRO is its derivation based on the
theoretically-justified algorithm [1] to do <strong>many rounds of
reasonable approximations</strong> to make the optimizing objective be
calculated effectively. Most importantly, the paper proves the proposed
algorithm is <strong>guaranteed to be improved monotonically</strong>.
TRPO is a model-free, policy-based, <strong>on-policy</strong>,
<strong>Monte Carlo</strong> algorithm that can be used in environments
with <strong>continuous state space and action space</strong>, and also
support high-dimensional input and large nonlinear policies such as
neural networks. The key idea is how to do approximation of the
optimization objective. The paper conducts experiments in a wide range
of tasks, including the robotic and Atari games domains, which shows
great performance.</p>
<h1 id="main-approximation-procedure">Main Approximation Procedure</h1>
<p>In this section, we will show how does the paper approximate the
optimization objective step-by-step. In the following sub-sections, the
notions used in this paper is a bit different from common ones. To
define the MDP model, it doesn't use the reward function, but uses the
<strong>cost function</strong> <span class="math inline">\(c:S
\rightarrow \mathbb{R}\)</span>, so the main target is to
<strong>minimize the expected cost</strong>, defined as <span
class="math inline">\(\eta(\pi)=\mathbb{E}[\sum_{t=0}^{\infty}{\gamma^t
c(s_t)}]\)</span>.</p>
<h2 id="preliminaries">Preliminaries</h2>
<p>Overall, we know the following basic concepts:</p>
<p><span class="math display">\[\begin{equation}
Q_{\pi}(s_t,a_t)=\mathbb{E}[\sum_{l=0}^{\infty}{\gamma^l c(s_{t+l})}],
a_t \sim \pi(s_t)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
V_{\pi} (s_t)=\mathbb{E}[\sum_{l=0}^{\infty}{\gamma^l c(s_{t+l})}]
\end{equation}\]</span></p>
<p>We also have the advantage function:</p>
<p><span class="math display">\[\begin{equation}
A_{\pi}(s,a)=Q_{\pi}(s,a)−V_{\pi}(s)
\end{equation}\]</span></p>
<p>Kakade &amp; Lanford (2002) [1] derived the relationship of the
expected discounted cost between two policies <span
class="math inline">\(\pi\)</span> and <span
class="math inline">\(\tilde{\pi}\)</span> to have the following
equation:</p>
<p><span class="math display">\[\begin{equation}
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_0,a_0,s_1,a_1...}[\sum_{t=0}^{\infty}{\gamma^t
A_{\pi}(s_t,a_t)}]
\end{equation}\]</span></p>
<p>Where the transitions are generated based on <strong>the
policy</strong> <span class="math inline">\(\tilde{\pi}\)</span>.</p>
<p>This is very straightforward, the actions are selected from policy
<span class="math inline">\(\tilde{\pi}\)</span> but the advantages are
computed by policy <span class="math inline">\(\pi\)</span>.</p>
<p>The paper introduced the <strong>(unnormalized) discounted visitation
frequencies</strong> as:</p>
<p><span class="math display">\[\begin{equation}
\rho_{\pi}(s)=P(s_0=s)+\gamma P(s_1=s)+...+\gamma^{\tau}
P(s_{\tau}=s)+...
\end{equation}\]</span></p>
<p>So the expected discounted cost can be rearranged as:</p>
<p><span class="math display">\[\begin{equation}
\eta(\tilde{\pi})=\eta(\pi)+\sum_{s}{\rho_{\tilde{\pi}}(s)
\sum_{a}{\tilde{\pi}(a│s) A_{\pi}(s,a)}}\end{equation}\]</span></p>
<p>In this case, we can see that, the cost function for the new policy
<span class="math inline">\(\tilde{\pi}\)</span> equals to adding an
additional term to the original cost function, so if the term <span
class="math inline">\(\sum_{s}{\rho_{\tilde{\pi}}(s)
\sum_{a}{\tilde{\pi}(a│s) A_{\pi}(s,a)}}\)</span> <strong>guarantees to
be a negative value</strong>, then the update can <strong>guarantee to
reduce the cost function</strong> <span
class="math inline">\(\eta\)</span>.</p>
<p>All of the following derivations are based on this formula, and many
rounds of approximations are done from it.</p>
<h2 id="mathematically-approximations">Mathematically
Approximations</h2>
<p>Firstly, the paper defines the <span
class="math inline">\(\eta(\tilde{\pi})\)</span> as the main obejctive
to minimize, denoted as <span
class="math inline">\(L_{\pi}(\tilde{\pi})\)</span>:</p>
<p><span class="math display">\[\begin{equation}
L_{\pi}(\tilde{\pi})=\eta(\pi)+\sum_{s}{\rho_{\tilde{\pi}}(s)
\sum_{a}{\tilde{\pi}(a│s) A_{\pi}(s,a)}}
\end{equation}\]</span></p>
<ul>
<li><p>The <strong><em>first approximation</em></strong>: As the
distribution of the states under a new policy is difficult to get, so it
replaces the new policy <strong>with the old policy</strong>, which
ignores the changes in state visitation density due to changes in the
policy: <span class="math display">\[\begin{equation}
L_{\pi}(\tilde{\pi})=\eta(\pi)+\sum_{s}{\rho_{\pi}(s)
\sum_{a}{\tilde{\pi}(a│s) A_{\pi}(s,a)}}
\end{equation}\]</span> So in this case, we use the old policy <span
class="math inline">\(\pi\)</span> for the state-visitation
distribution.</p></li>
<li><p>Then based on the <strong>conservative policy iteration</strong>:
<span class="math display">\[\begin{equation}
\pi_{new} (a│s)=(1−\alpha)\pi_{old}(a│s)+\alpha \pi&#39;(a|s)
\end{equation}\]</span> And the inequation that Kakade and Langford
proved: <span class="math display">\[\begin{equation}
\eta(\pi_{new}) \leq L_{\pi_{old}}(\pi_{new})+ \frac{2\epsilon
\gamma}{(1−\gamma(1−\alpha))(1−\gamma)\alpha^2}
\end{equation}\]</span> Where <span
class="math inline">\(\epsilon=\max_s{⁡|\mathbb{E}_{a \sim \pi&#39;
(a│s)} [A_{\pi}(s,a)]|}\)</span></p>
<p>The <strong><em>second approximation</em></strong>: As <span
class="math inline">\(\alpha \ll 1\)</span>, we can approximate the
above inequation as:</p>
<p><span class="math display">\[\begin{equation}
\eta(\pi_{new}) \leq L_{\pi_{old}}(\pi_{new})+\frac{2 \epsilon
\gamma}{(1−\gamma)^2 \alpha^2}
\end{equation}\]</span></p></li>
</ul>
<h2
id="from-conservative-policy-iteration-to-general-stochastic-policies">From
Conservative Policy Iteration to General Stochastic Policies</h2>
<p>The second approximation is based on the conservative policy
iteration, however, which is not commonly used in a wide range of
problems, so make the algorithm applicable to more general stochastic
policies methods, the crucial is to eliminate the <strong>step
factor</strong> to extend the ineuqation (the guarantee) to practical
problems.</p>
<ul>
<li>The <strong><em>third approximation</em></strong>: replace <span
class="math inline">\(\alpha\)</span> with a distance measure between
<span class="math inline">\(\pi\)</span> and <span
class="math inline">\(\tilde{\pi}\)</span>. The paper uses the
<strong>total variance divergence</strong> to replace <span
class="math inline">\(\alpha\)</span>: <span
class="math display">\[\begin{equation}
\alpha=D_{TE}^{max}(\pi_{old},\pi_{new})
\end{equation}\]</span></li>
<li>The <strong><em>fourth approximation</em></strong>: replace the
total variance divergence with the <strong>KL divergence</strong> based
on their unequal relationship <span class="math inline">\(D_{TV}(p||q)^2
\leq D_{K}(p||q)\)</span>: <span class="math display">\[\begin{equation}
\alpha=D_{KL}^{max}(\pi_{old},\pi_{new})
\end{equation}\]</span></li>
</ul>
<p>For now, based on the finally derived objective to be minimized, the
algorithm can be shown as</p>
<figure>
<img src="/images/RLPapers/Model-Free/PolicyGradients/2-2.png"
alt="Algorithm: Approximate policy iteration algorithm guaranteeing non-increasing expected cost" />
<figcaption aria-hidden="true">Algorithm: Approximate policy iteration
algorithm guaranteeing non-increasing expected cost</figcaption>
</figure>
<p>The algorithm provides the theorical formula to update the policies
based on the assumption that we know all of the transition functions and
we can directly compute the <span class="math inline">\(argmin\)</span>
part. However, we usually have no exact form of the policies, which
means we <strong>cannot directly compute the</strong> <span
class="math inline">\(argmin\)</span>, so in the following sections, we
consider the <strong>parameterized</strong> policy function <span
class="math inline">\(\pi_{\theta}\)</span>.</p>
<h2 id="optimization-of-parameterized-policies">Optimization of
Parameterized Policies</h2>
<p>In this section, we consider the <strong>parameterized</strong>
policy function <span class="math inline">\(\pi_{\theta}\)</span> to see
how can we directly update the parameters from <span
class="math inline">\(\theta_{old}\)</span> to <span
class="math inline">\(\theta\)</span>. To rewrite the objective in a
parameterized form, we have:</p>
<p><span class="math inline">\(\eta(\theta) &lt;
L_{\theta_{old}}(\theta)+C
D_{KL}^{max}(\theta_{old},\theta)\)</span></p>
<ul>
<li>The <strong><em>fifth approximation</em></strong>: to use a
constraint on the KL divergence between the new policy and the old
policy, i.e., a <strong>trust region constraint</strong>: <span
class="math display">\[\begin{equation}
\min_{\theta}[L_{\theta_{old}}(\theta)]
\end{equation}\]</span> <span class="math display">\[\begin{equation}
s.t. D_{KL}^{max}(\theta_{old},\theta)\leq \delta
\end{equation}\]</span></li>
<li>The <strong><em>sixth approximation</em></strong>: as the <span
class="math inline">\(max\)</span> operator makes the optimizaion hard,
so we can use the average KL divergence instead of the maximal: <span
class="math display">\[\begin{equation}
\min_{\theta}[L_{\theta_{old}}(\theta)]
\end{equation}\]</span> <span class="math display">\[\begin{equation}
s.t. D_{KL}^{\rho_{\theta_{old}}}(\theta_{old},\theta) \leq \delta
\end{equation}\]</span></li>
</ul>
<h2 id="sample-based-estimation">Sample-Based Estimation</h2>
<p>Now we have had the theoretical formula, but in practice, we cannot
compute the exact value of the main objective and the constraint, we
usually need to estimate these values by sampling, so in this section,
the paper introduced how to use <strong><em>importance
sampling</em></strong> to estimate the two values.</p>
<ul>
<li>The <strong><em>seventh approximation</em></strong>: to use
importance sampling to estimate the expectation term: <span
class="math display">\[\begin{equation}
\sum_{a}{\pi_{\theta}(a│s_n)A_{\pi_{\theta}}(s_n,a)} \approx
\mathbb{E}_{a \sim q} \Big[\frac{\pi_{\theta}(a│s)}{q(a│s)}
A_{\theta_{old}}(s,a)\Big]
\end{equation}\]</span></li>
<li>The <strong><em>eighth approximation</em></strong>: to avoid
computing the advantage functions, just replace it using the
Q-functions, so the final objective to optimize is: <span
class="math display">\[\begin{equation}
\min_{\theta}⁡{\mathbb{E}_{s \sim \rho_{\theta_{old}}, a \sim
q}\Big[\frac{\pi_{\theta}(a│s)}{q(a│s)} A_{\theta_{old}}(s,a)\Big]}
\end{equation}\]</span> <span class="math display">\[\begin{equation}
s.t. \mathbb{E}_{s \sim \rho_{\theta_{old}}} [D_{KL}
(\pi_{\theta_{old}}(\cdot│s)||\pi_{\theta} (\cdot|s))] \leq \delta
\end{equation}\]</span></li>
</ul>
<h2 id="two-variant-sampling-approaches">Two Variant Sampling
Approaches</h2>
<p>The paper further proposed two variant sampling approaches:</p>
<ul>
<li><strong>Single Path</strong>: typically used for policy estimation
and is based on sampling individual trajectories.</li>
<li><strong>Vine</strong>: construct a rollout set and then perform
multiple actions from each sate in the rollout set.</li>
</ul>
<figure>
<img src="/images/RLPapers/Model-Free/PolicyGradients/2-3.png"
alt="Two variant sampling approaches" />
<figcaption aria-hidden="true">Two variant sampling
approaches</figcaption>
</figure>
<h1 id="reference">Reference</h1>
<p>[1] Sham Kakade and John Langford. 2002. Approximately Optimal
Approximate Reinforcement Learning. In Proceedings of the Nineteenth
International Conference on Machine Learning (ICML ’02), Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA, 267–274.</p>
]]></content>
      <categories>
        <category>RL Paper Notes</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Paper Notes</tag>
        <tag>Model-Free RL</tag>
        <tag>Policy Gradients</tag>
      </tags>
  </entry>
  <entry>
    <title>Use PySC2 Learning Environment - Define Your Own Agent</title>
    <url>/2022/2-tutorial-pysc2-2/</url>
    <content><![CDATA[<p>This is a brief introduction to an easy approach to use the
<code>PySC2</code> by defining your own agent. A basic template for a
self-defined agent can be found <a
href="https://github.com/mhz2180572509/HowToUsePySC2/blob/main/Approach_1.py">here</a>.
(The codes are also attached at the end).</p>
<ul>
<li>For an overview of <code>PySC2</code>, you can proceed to
<a href="/2022/1-tutorial-pysc2-1/" title="A Brief Tutorial to Use PySC2 Learning Environment (Overview)">A Brief Tutorial to Use PySC2 Learning Environment (Overview)</a>.</li>
<li>For an more comprehensive approach to define your own environment
and agent, and also control the interaction between them, you can
proceed to <a href="/2022/3-tutorial-pysc2-3/" title="Use PySC2 Learning Environment - Control Your Own Environment and Agent">Use PySC2 Learning Environment - Control Your Own Environment and Agent</a></li>
</ul>
<span id="more"></span>
<h1 id="how-to-use-custom-defined-agents">How to Use Custom-Defined
Agents?</h1>
<p>Run the command:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m pysc2.bin.agent --map &lt;Map&gt; --agent &lt;Agent&gt;</span><br></pre></td></tr></table></figure>
<p>Assuming that your own agent class named
<code>MyAgentClassName</code> is defined in <code>MyAgentFile.py</code>
in a directory <code>[PATH]</code>, then when you run command in the
<code>[PATH]</code> directory, you can identify the agent by:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m pysc2.bin.agent --map &lt;Map&gt; --agent MyAgentFile.MyAgentClassName</span><br></pre></td></tr></table></figure>
<h1 id="how-to-design-my-own-agent-and-make-it-interact-with-pysc2">How
to Design My Own Agent and Make It Interact with PySC2?</h1>
<p>Any custom designed agent should derive from the
<code>BaseAgent</code> class (in <code>pysc2.agents.base_agent</code>)
and override the <code>step(self, obs)</code> function, where the
argument <code>obs</code> is the observations.</p>
<h1 id="how-to-get-observations-and-rewards-from-the-environment">How to
Get Observations and Rewards from The Environment?</h1>
<p>The observations (as well as the rewards) taken from environment are
all included in the parameter <code>obs</code> of the <code>step</code>
function, such as the feature maps, valid actions, rewards and so
on.</p>
<p>for example: * state, viz. different types of feature maps:
<code>obs.observation["screen"][feature_map_name]</code> * valid
actions: <code>obs.observation["available_actions"]</code> or
<code>obs.observation.available_actions</code> * rewards:
<code>obs.reward</code></p>
<h1 id="how-to-send-actions-to-interact-with-the-environment">How to
Send Actions to Interact with The Environment?</h1>
<p>In the <code>step</code> function, return the action (packed up as a
<code>FunctionCall</code> instance).</p>
<h1 id="more-reference">More Reference</h1>
<ul>
<li>A random agent: <a
href="https://github.com/deepmind/pysc2/blob/master/pysc2/agents/random_agent.py"
class="uri">https://github.com/deepmind/pysc2/blob/master/pysc2/agents/random_agent.py</a></li>
<li>A scripted agent: <a
href="https://github.com/deepmind/pysc2/blob/master/pysc2/agents/scripted_agent.py"
class="uri">https://github.com/deepmind/pysc2/blob/master/pysc2/agents/scripted_agent.py</a></li>
</ul>
<h1 id="template-codes">Template Codes</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">A template for approach 1.</span></span><br><span class="line"><span class="string">The agent randomly selects actions.</span></span><br><span class="line"><span class="string">Use the `pysc2.bin.agent` module to run `MyAgent` when your commands are in the current path:</span></span><br><span class="line"><span class="string">    python -m pysc2.bin.agent --map &lt;MAP_NAME&gt; --agent pysc2EnvTemplate.MyAgent</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># to be compatible with python 2.x</span></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># import the BaseAgent class which we should derive from</span></span><br><span class="line"><span class="keyword">from</span> pysc2.agents <span class="keyword">import</span> base_agent</span><br><span class="line"><span class="comment"># import actions</span></span><br><span class="line"><span class="keyword">from</span> pysc2.lib <span class="keyword">import</span> actions</span><br><span class="line"><span class="comment"># import features</span></span><br><span class="line"><span class="keyword">from</span> pysc2.lib <span class="keyword">import</span> features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define our own agent</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAgent</span>(<span class="params">base_agent.BaseAgent</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, obs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyAgent, self).step(obs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># -------------------#</span></span><br><span class="line">        <span class="comment"># RL algorithm here~ #</span></span><br><span class="line">        <span class="comment"># -------------------#</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get available actions</span></span><br><span class="line">        function_id = np.random.choice(obs.observation.available_actions)</span><br><span class="line">        <span class="comment"># randomly select one action and its argument</span></span><br><span class="line">        args = [[np.random.randint(<span class="number">0</span>, size) <span class="keyword">for</span> size <span class="keyword">in</span> arg.sizes]</span><br><span class="line">                <span class="keyword">for</span> arg <span class="keyword">in</span> self.action_spec.functions[function_id].args]</span><br><span class="line">        <span class="comment"># pack up the actions as a `FunctionCall`</span></span><br><span class="line">        function_call = actions.FunctionCall(function_id, args)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># return the actions a.k.a. the function_call</span></span><br><span class="line">        <span class="keyword">return</span> function_call</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tutorial</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>PySC2</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title>Use PySC2 Learning Environment - Control Your Own Environment and Agent</title>
    <url>/2022/3-tutorial-pysc2-3/</url>
    <content><![CDATA[<p>This is a brief introduction to an more comprehensive approach to use
the <code>PySC2</code> by defining both your own environment and agent,
and you can also control the interaction between the agent and the
environment. A basic template can be found <a
href="https://github.com/mhz2180572509/HowToUsePySC2/blob/main/Approach_2.py">here</a>.
(The codes are also attached at the end).</p>
<ul>
<li>For an overview of <code>PySC2</code>, you can proceed to
<a href="/2022/1-tutorial-pysc2-1/" title="A Brief Tutorial to Use PySC2 Learning Environment (Overview)">A Brief Tutorial to Use PySC2 Learning Environment (Overview)</a>.</li>
<li>For an simpler approach to only define your own agent, you can
proceed to <a href="/2022/2-tutorial-pysc2-2/" title="Use PySC2 Learning Environment - Define Your Own Agent">Use PySC2 Learning Environment - Define Your Own Agent</a></li>
</ul>
<span id="more"></span>
<p>The environment of PySC2 is defined in
<code>pysc2.env.sc2_env</code>, while the actions and observations are
defined in <code>pysc2.lib.features</code>. The class of
<code>PySC2</code> environment is <code>SC2Env</code>, which Inherits
from <code>pysc2.env.environment.Base</code>. <strong>You can use the
<code>SC2Env</code> just like any other <em>OpenAI Gym</em>
environments.</strong></p>
<p>A basic template to instantiate the environment and an agent, and run
the codes to make the agent interact with the environment can be found
<a
href="https://github.com/mhz2180572509/HowToUsePySC2/blob/main/Approach_1.py">here</a>.</p>
<h1 id="to-instantiate-an-environment">To Instantiate An
Environment</h1>
<p>Import the <code>SC2Env</code> from <code>pysc2.env.sc2_env</code>
and instantiate it by <code>env = SC2Env([parameters])</code>.</p>
<p>The definition of the class and all its parameters can be found <a
href="https://github.com/deepmind/pysc2/blob/master/pysc2/env/sc2_env.py#L121">here</a>.</p>
<p>Some important arguments:</p>
<ul>
<li><code>map_name</code>: the name of the map.
<ul>
<li>You can find the namelist in <code>pysc2.bin.map_list</code> or by
running <code>python -m pysc2.bin.map_list</code></li>
<li>Or you can find them under the <code>pysc2/maps/</code> folder, like
the script <code>pysc2/maps/melee.py</code> lists all the maps of
<em>Melee</em>: <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">melee_maps = [&quot;Flat32&quot;, &quot;Flat48&quot;, &quot;Flat64&quot;, &quot;Flat96&quot;, &quot;Flat128&quot;, &quot;Simple64&quot;, &quot;Simple96&quot;, &quot;Simple128&quot;,]</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><code>players</code>：a list, containing one or two instances of
<code>pysc2.env.sc2_env.Agent</code> or
<code>pysc2.env.sc2_env.Bot</code>, some maps only allow one agent.
<ul>
<li>Note: the <code>Agent</code> and <code>Bot</code> is different from
the <code>pysc2.agents.base_agent</code> we mentioned before, here is an
example for these two classes: <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>(<span class="params">collections.namedtuple(<span class="params"><span class="string">&quot;Agent&quot;</span>, [<span class="string">&quot;race&quot;</span>, <span class="string">&quot;name&quot;</span>]</span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Define an Agent. It can have a single race or a list of races.&quot;&quot;&quot;</span></span><br><span class="line">	</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, race, name=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>(Agent, cls).__new__(cls, to_list(race), name <span class="keyword">or</span> <span class="string">&quot;&lt;unknown&gt;&quot;</span>)</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bot</span>(<span class="params">collections.namedtuple(<span class="params"><span class="string">&quot;Bot&quot;</span>, [<span class="string">&quot;race&quot;</span>, <span class="string">&quot;difficulty&quot;</span>, <span class="string">&quot;build&quot;</span>]</span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Define a Bot. It can have a single or list of races or builds.&quot;&quot;&quot;</span></span><br><span class="line">	</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span>(<span class="params">cls, race, difficulty, build=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">super</span>(Bot, cls).__new__(cls, to_list(race), difficulty, to_list(build <span class="keyword">or</span> BotBuild.random))</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><code>agent_interface_format</code>: to define the format of
observation and action, like the resolution of the feature maps. It
takes an instance of
<code>pysc2.lib.features.AgentInterfaceFormat</code> or an instance of
<code>pysc2.env.sc2_env.AgentInterfaceFormat</code>.
<ul>
<li>The number of <code>AgentInterfaceFormat</code> should be the same
as the length of the list for the above <code>player</code> argument,
with the same order.</li>
<li>Please refer to <a
href="https://github.com/deepmind/pysc2/blob/master/pysc2/lib/features.py#L470">the
definition</a> of <code>AgentInterfaceFormat</code> for more
details.</li>
</ul></li>
<li><code>step_mul</code>: an int to identify after how many steps to
take one observation and apply one action. (not the real number of
frames). 1 second equals to 16 steps, which means if we set 16 here, we
will get the observations every 1 second.</li>
<li><code>game_steps_per_episode</code>: how many steps of one episode.
For example, if we set <code>200*16</code> here, it means one episode
will last for 200 seconds.</li>
<li><code>save_replay_episodes</code>: how many episodes to save one
replay.</li>
<li><code>replay_dir</code>: the location to save replays.</li>
</ul>
<h1 id="define-a-function-to-interact-with-the-environment">Define A
Function to Interact with The Environment</h1>
<p>To define a function to interact with the environment, you can refer
to the <code>run_loop</code> function in <a
href="https://github.com/deepmind/pysc2/blob/master/pysc2/env/run_loop.py#L23">pysc2/env/run_loop.py</a></p>
<p>Here is also a good reference to instantiate an <code>SC2Env</code>
and an <code>Agent</code>, and use the above <code>run_loop</code>
function to test them in <a
href="https://github.com/deepmind/pysc2/blob/master/pysc2/tests/easy_scripted_test.py">pysc2/tests/easy_scripted_test.py</a>.</p>
<h1 id="run-the-program">Run the Program</h1>
<p>To actually run the codes above, we need the <code>run</code>
function in <code>absl.app</code> package. The function takes in our
running function as a parameter, like this:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pysc2.env <span class="keyword">import</span> run_loop, sc2_env</span><br><span class="line"><span class="keyword">from</span> pysc2.agents <span class="keyword">import</span> random_agent</span><br><span class="line"><span class="keyword">from</span> absl <span class="keyword">import</span> app</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">args</span>):</span></span><br><span class="line">    agent = random_agent.RandomAgent()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> sc2_env.SC2Env(map_name=<span class="string">&quot;MoveToBeacon&quot;</span>, </span><br><span class="line">	                players=[sc2_env.Agent(sc2_env.Race.terran)],</span><br><span class="line">			agent_interface_format=sc2_env.AgentInterfaceFormat(feature_dimensions=sc2_env.Dimensions(screen=<span class="number">84</span>, minimap=<span class="number">64</span>)), </span><br><span class="line">			step_mul=<span class="number">16</span>,</span><br><span class="line">			game_steps_per_episode=<span class="number">200</span> * <span class="number">16</span>, </span><br><span class="line">			visualize=<span class="literal">True</span>) <span class="keyword">as</span> env:</span><br><span class="line">	run_loop.run_loop([agent], env, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    app.run(main)</span><br></pre></td></tr></table></figure>
<p><strong>Notes</strong>: When we define the <code>main</code>
function, it must have at lest one argument, like <code>args</code>
here. You may not use it, but you have to define it.</p>
<h1 id="template-codes">Template Codes</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">A template  for approach 2.</span></span><br><span class="line"><span class="string">Agent: using the random agent, referring to: pysc2.agent.random_agent.RandomAgent</span></span><br><span class="line"><span class="string">Environment: using the &#x27;Simple64&#x27; environment, both the player and the build-in AI are the race of terran.</span></span><br><span class="line"><span class="string">You can simply run the agent in the environment by the command:</span></span><br><span class="line"><span class="string">    python Approach_2.py</span></span><br><span class="line"><span class="string">Or you can use the `pysc2.bin.agent` module to run the MyAgent individually like the approach 1:</span></span><br><span class="line"><span class="string">    python -m pysc2.bin.agent --map &lt;MAP_NAME&gt; --agent Approach_2.MyAgent</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># import the BaseAgent class which we should derive from</span></span><br><span class="line"><span class="keyword">from</span> pysc2.agents <span class="keyword">import</span> base_agent</span><br><span class="line"><span class="comment"># import actions</span></span><br><span class="line"><span class="keyword">from</span> pysc2.lib <span class="keyword">import</span> actions</span><br><span class="line"><span class="comment"># import features</span></span><br><span class="line"><span class="keyword">from</span> pysc2.lib.features <span class="keyword">import</span> AgentInterfaceFormat, Dimensions</span><br><span class="line"><span class="comment"># import the SC2Env environment</span></span><br><span class="line"><span class="keyword">from</span> pysc2.env.sc2_env <span class="keyword">import</span> SC2Env, Agent, Bot, Race, Difficulty</span><br><span class="line"><span class="comment"># import absl</span></span><br><span class="line"><span class="keyword">from</span> absl <span class="keyword">import</span> app</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the agent, overriding the `step` function</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyAgent</span>(<span class="params">base_agent.BaseAgent</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    My customized agent, simply copied from `pysc2.agents.random_agent.RandomAgent`.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, obs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyAgent, self).step(obs)</span><br><span class="line">        function_id = np.random.choice(obs.observation.available_actions)</span><br><span class="line">        args = [[np.random.randint(<span class="number">0</span>, size) <span class="keyword">for</span> size <span class="keyword">in</span> arg.sizes] <span class="keyword">for</span> arg <span class="keyword">in</span></span><br><span class="line">                self.action_spec.functions[function_id].args]</span><br><span class="line">        <span class="keyword">return</span> actions.FunctionCall(function_id, args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define a loop function for only one agent</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loop</span>(<span class="params">agent, env, max_steps=<span class="number">200</span></span>):</span></span><br><span class="line">    <span class="comment"># Set up the agent</span></span><br><span class="line">    <span class="comment"># ! Important: here, to setup the agent, the `env.observation_spec()` and `env.action_spec()` return tuples,</span></span><br><span class="line">    <span class="comment"># ! however, we want the elements inside, so we need the indexes `[0]` form them respectively.</span></span><br><span class="line">    observation_spec = env.observation_spec()[<span class="number">0</span>]</span><br><span class="line">    action_spec = env.action_spec()[<span class="number">0</span>]</span><br><span class="line">    agent.setup(observation_spec, action_spec)</span><br><span class="line">    <span class="comment"># reset the environment and the agent</span></span><br><span class="line">    <span class="comment"># ! Note: the `env.reset()` returns a tuple,</span></span><br><span class="line">    <span class="comment"># ? each element for each agent/bot.</span></span><br><span class="line">    timesteps = env.reset()</span><br><span class="line">    agent.reset()</span><br><span class="line">    total_steps = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># ! Note: the `env.step()` function needs a list, which actually allows multiple actions per frame,</span></span><br><span class="line">        <span class="comment"># ! so, we need to pack up the single action returned by the `agent.step()` function.</span></span><br><span class="line">        <span class="comment"># ! However, if the `agent.step()` function returns a list itself, then there is no need to use the list.</span></span><br><span class="line">        action = [agent.step(timesteps[<span class="number">0</span>])]</span><br><span class="line">        <span class="keyword">if</span> timesteps[<span class="number">0</span>].last():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        total_steps += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_steps &gt; max_steps:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        timesteps = env.step(action)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># define the main function</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>(<span class="params">args</span>):</span></span><br><span class="line">    agent = MyAgent()</span><br><span class="line">    <span class="comment"># define the environment</span></span><br><span class="line">    env = SC2Env(map_name=<span class="string">&quot;Simple64&quot;</span>,</span><br><span class="line">                 players=[Agent(Race.terran), Bot(Race.terran, Difficulty.very_easy)],</span><br><span class="line">                 agent_interface_format=AgentInterfaceFormat(feature_dimensions=Dimensions(screen=<span class="number">84</span>, minimap=<span class="number">64</span>)),</span><br><span class="line">                 step_mul=<span class="number">16</span>, visualize=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        loop(agent, env)</span><br><span class="line">    <span class="keyword">except</span> KeyboardInterrupt:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Finished!&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    app.run(main)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Tutorial</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>PySC2</tag>
        <tag>Tools</tag>
      </tags>
  </entry>
  <entry>
    <title>Poisson Distribution - Expectation, Variance, Likelihood and Regression</title>
    <url>/2022/4-ProbabilisticML-1-Poisson/</url>
    <content><![CDATA[<p>Poisson distribution can be used to express count data, which gives
number of events occurring in a fixed interval of time or space and
these events occur with a known constant mean rate and independently of
the time since the last event.</p>
<p>This blog aims to show the detailed proof of its expectation,
variance, likelihood and regression.</p>
<span id="more"></span>
<h1 id="probability-density-formula">Probability Density Formula</h1>
<p>Suppose a discrete random variable <span
class="math inline">\(X\)</span> of non-negative integer numbers, and
let <span
class="math inline">\(\lambda=\mu(x;\beta)=e^{x\beta}\geq0\)</span>.
Then we can say:</p>
<p><span class="math display">\[\begin{equation}
Y \sim Poisson(\mu(x,\beta))
\end{equation}\]</span></p>
<p>if:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
P(Y=y_i) &amp;= e^{-\lambda_i} \frac{\lambda_i^{y_i}}{y_i!}\\
&amp;= e^{-\mu(x_i,\beta)} \frac{\mu(x_i,\beta)^{y_i}}{y_i!}
\end{aligned}
\end{equation}\]</span></p>
<h1 id="expectation-and-variance">Expectation and Variance</h1>
<p>The expectation and the variation of Poisson distribution both
numerically equals to <span class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{E}(Y)=\mathrm{Var}(Y)=\lambda=\mu(x;\beta)
\end{equation}\]</span></p>
<h2 id="expectation">Expectation</h2>
<p>To proof:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mathrm{E}(Y) &amp;= \sum_{y=0}^{\infty}{y e^{-\lambda}
\frac{\lambda^{y}}{y!}} \\
&amp;= \sum_{y=1}^{\infty}{y e^{-\lambda} \frac{\lambda^{y}}{y!}} \\
&amp;= \sum_{y=1}^{\infty}{e^{-\lambda} \frac{\lambda^{y -
1}\lambda}{(y-1)!}} \\
&amp;= \lambda e^{-\lambda} \sum_{y=1}^{\infty}{\frac{\lambda^{y -
1}}{(y-1)!}}
\end{aligned}
\end{equation}\]</span></p>
<p>Based on the Taylor expansion:</p>
<p><span class="math display">\[\begin{equation}
e^x = 1 + x+\frac{x^2}{2!}+...+\frac{x^n}{n!}+... =
\sum_{k=1}^{\infty}{\frac{x^{k-1}}{(k-1)!}}
\end{equation}\]</span></p>
<p>We have:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{E}(Y) = \lambda e^{-\lambda}
\sum_{y=1}^{\infty}{\frac{\lambda^{y - 1}}{(y-1)!}} = \lambda
e^{-\lambda} e^{\lambda} = \lambda
\end{equation}\]</span></p>
<h2 id="variance">Variance</h2>
<p>To compute the variance, we firstly compute the expectation of <span
class="math inline">\(Y^2\)</span>:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mathrm{E}(Y^2) &amp;= \sum_{y=0}^{\infty}{y^2 e^{-\lambda}
\frac{\lambda^y}{y!}} \\
&amp;= \lambda e^{-\lambda} \sum_{y=1}^{\infty}{\frac{y \lambda^{y -
1}}{(y-1)!}} \\
&amp;= \lambda e^{-\lambda} \sum_{y=1}^{\infty}{\frac{(y-1+1)\lambda^{y
- 1}}{(y-1)!}}
\end{aligned}
\end{equation}\]</span></p>
<p>Set <span class="math inline">\(m=k-1\)</span>, we have:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\mathrm{E}(Y^2) &amp;= \lambda e^{-\lambda}(\sum_{m=0}^{\infty}{\frac{m
\lambda^m}{m!}} + \sum_{m=0}^{\infty}{\frac{\lambda^m}{m!}}) \\
&amp;= \lambda e^{-\lambda}(\lambda
\sum_{m=1}^{\infty}{\frac{\lambda^{m-1}}{(m-1)!}} +
\sum_{m=0}^{\infty}{\frac{\lambda^m}{m!}}) \\
&amp;= \lambda e^{-\lambda} (\lambda e^{\lambda} +e^{\lambda} ) =
\lambda(\lambda + 1)
\end{aligned}
\end{equation}\]</span></p>
<p>Finally, we can derive:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{Var}(Y) = \mathrm{E}(Y^2) - (\mathrm{E}(Y))^2 = \lambda(\lambda
+ 1) - {\lambda}^2 = \lambda
\end{equation}\]</span></p>
<h1 id="log-likelihood">Log-Likelihood</h1>
<p>Then we can derive the likelihood of <span
class="math inline">\(Y\)</span> is:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
L(y;\beta) &amp;= \prod_{i=1}^{n}{P(Y=y_i)} \\
&amp;= \prod_{i=1}^{n}{e^{-\mu(x_i;\beta)}}\prod_{i=1}^n
\frac{\mu(x_i;\beta)^{y_i}}{y_i!}\\
&amp;=
e^{-\sum_{i=1}^{n}{\mu(x_i;\beta)}}\prod_{i=1}^{n}{\frac{\mu(x_i;\beta)^{y_i}}{y_i!}}
\end{aligned}
\end{equation}\]</span></p>
<p>Then, we can get the log-likelihood as:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\ln{L(y;\beta)} &amp;=
-\sum_{i=1}^{n}{\mu(x_i;\beta)}+\sum_{i=1}^{n}{y_i
\ln{\mu(x_i;\beta)}}-\sum_{i=1}^{n}{\ln{y_i!}}\\
&amp;= -\sum_{i=1}^{n}{\mu(x_i;\beta)}+\sum_{i=1}^{n}{y_i
\ln{\mu(x_i;\beta)}} + const
\end{aligned}
\end{equation}\]</span></p>
<h1 id="poisson-regression">Poisson Regression</h1>
<p>We can learn the parameters <span
class="math inline">\(\beta\)</span> based on <em>MLE</em> to do
regression. The following codes are based on
<code>scipy.optimize.minimize</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize <span class="keyword">as</span> mm</span><br><span class="line"></span><br><span class="line">N = <span class="number">10000</span>  <span class="comment"># number of data points</span></span><br><span class="line">P = <span class="number">5</span>  <span class="comment"># number of features</span></span><br><span class="line"></span><br><span class="line">X = np.random.randn(N, P)  <span class="comment"># matrix X in shape of NxP</span></span><br><span class="line">beta_real = np.random.randn(P)  <span class="comment"># the real values of \beta, in shape of P</span></span><br><span class="line">Y = np.random.poisson(np.exp(np.dot(X, beta_real)))  <span class="comment"># here, \lambda = exp(X\beta)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># The Poisson distribution of np.random.poisson(lam):</span></span><br><span class="line"><span class="comment"># f(k; lam) = (lam^k exp(-lam)) / (k!)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss function to maximize the loge-likelihood</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">beta</span>):</span></span><br><span class="line">    Xbeta = np.dot(X, beta)</span><br><span class="line">    exp_Xbeta = np.exp(Xbeta)  <span class="comment"># \lambda = exp(X beta)</span></span><br><span class="line">    loss = exp_Xbeta.<span class="built_in">sum</span>() - np.dot(Y, Xbeta)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># optimize with scipy.optimize.minimize</span></span><br><span class="line">beta_ini = np.zeros(P)  <span class="comment"># initialize beta with zeros</span></span><br><span class="line">result = mm(loss_function, beta_ini)</span><br><span class="line">beta_predicted = result.x</span><br><span class="line"></span><br><span class="line"><span class="comment"># show results</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Real beta: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(beta_real))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicted beta: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(beta_predicted))</span><br></pre></td></tr></table></figure>
<p>The output of the above codes:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Real beta: [-<span class="number">0.40914376</span>  <span class="number">0.29649545</span>  <span class="number">0.59736634</span>  <span class="number">1.40512766</span> -<span class="number">0.55024536</span>]</span><br><span class="line">Predicted beta: [-<span class="number">0.40346132</span>  <span class="number">0.29799245</span>  <span class="number">0.60520007</span>  <span class="number">1.40263191</span> -<span class="number">0.54896361</span>]</span><br></pre></td></tr></table></figure>
<h1 id="main-reference">Main Reference</h1>
<ul>
<li>Lecture slides by Claudia Czado: <a
href="https://www.groups.ma.tum.de/fileadmin/w00ccg/statistics/czado/lec6.pdf"
class="uri">https://www.groups.ma.tum.de/fileadmin/w00ccg/statistics/czado/lec6.pdf</a></li>
<li>Poisson distribution Wikipedia: <a
href="https://en.wikipedia.org/wiki/Poisson_distribution"
class="uri">https://en.wikipedia.org/wiki/Poisson_distribution</a></li>
<li>Poisson regression by <em>ahwillia</em>: <a
href="https://gist.github.com/ahwillia/40cdbe3b2f2df1806358dd1e6de0743a"
class="uri">https://gist.github.com/ahwillia/40cdbe3b2f2df1806358dd1e6de0743a</a></li>
</ul>
]]></content>
      <categories>
        <category>Probabilistic Machine Learning</category>
      </categories>
      <tags>
        <tag>Probability Theory</tag>
        <tag>Machine Learning</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>First Order AutoRegression (AR(1)) - Expectation, Variance, Likelihood and Regression</title>
    <url>/2022/6-ProbabilisticML-3-AR1/</url>
    <content><![CDATA[<p>This blog aims to show the detailed derivation of the expectation,
variance, likelihood and regression of <em>First-order autoregression
(AR(1))</em> model.</p>
<span id="more"></span>
<h1 id="probability-density-formula">Probability Density Formula</h1>
<p>The <em>first-order autoregression</em> (<em>AR(1)</em>) can model
(weakly) stationary time series, if the series <span
class="math inline">\(x_t\)</span> satisfy the following properties:</p>
<ul>
<li>The mean <span class="math inline">\(\mathrm{E}(x_t)\)</span> is the
same for all <span class="math inline">\(t\)</span>.</li>
<li>The variance <span class="math inline">\(\mathrm{Var}(x_t)\)</span>
is the same for all <span class="math inline">\(t\)</span>.</li>
<li>The covariance (and also correlation) between <span
class="math inline">\(x_t\)</span> and <span
class="math inline">\(x_{t-h}\)</span> is the same for all <span
class="math inline">\(t\)</span> at each lag <span
class="math inline">\(h=1,2,3\)</span>, etc.</li>
</ul>
<p>In the first-order autoregression model, the value of <span
class="math inline">\(x\)</span> at time <span
class="math inline">\(t\)</span> is a linear function of the value of
<span class="math inline">\(x\)</span> at time <span
class="math inline">\(t-1\)</span>, which can be written as:</p>
<p><span class="math display">\[\begin{equation}
x_t = \delta + \phi x_{t-1} + \omega_t
\end{equation}\]</span></p>
<p>with the following assumptions:</p>
<ul>
<li><span class="math inline">\(\omega_t \backsim
N(0,\sigma_{\omega}^2)\)</span> i.i.d..</li>
<li>Properties of the errors <span
class="math inline">\(\omega_t\)</span> are independent of <span
class="math inline">\(x_t\)</span>.</li>
<li>The series <span class="math inline">\(x_t\)</span> is (weakly)
stationary, which requires <span class="math inline">\(|\phi|\ &lt;
1\)</span>.</li>
</ul>
<h1 id="expectation-and-variance">Expectation and Variance</h1>
<h2 id="expectation">Expectation</h2>
<p>The expectation of AR(1) model is:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{E}(x_t)=\frac{\delta}{1-\phi}
\end{equation}\]</span></p>
<p>To proof:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{E}(x_t)=\mathrm{E}(\delta + \phi x_{t-1} +
\omega_t)=\mathrm{E}(\delta) + \mathrm{E}(\phi x_{t-1})+
\mathrm{E}(\omega_t) = \delta + \phi \mathrm{E}(x_{t-1})
\end{equation}\]</span></p>
<p>With the stationary assumption, <span
class="math inline">\(\mathrm{E}(x_t) = \mathrm{E}(x_{t-1})\)</span>, we
can solve the equation to get:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{E}(x_t)=\frac{\delta}{1-\phi}
\end{equation}\]</span>.</p>
<h2 id="variance">Variance</h2>
<p>The variance of AR(1) model is:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{Var}(x_t)=\frac{\sigma_{\omega}^2}{1-\phi^2}
\end{equation}\]</span></p>
<p>To proof:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{Var}(x_t)=\mathrm{Var}(\delta) + \mathrm{Var}(\phi x_{t-1})+
\mathrm{Var}(\omega_t) = \phi^2 \mathrm{Var}(x_{t-1}) +
\sigma_{\omega}^2
\end{equation}\]</span></p>
<p>With the stationary assumption, <span
class="math inline">\(\mathrm{Var}(x_t) =
\mathrm{Var}(x_{t-1})\)</span>, we can solve the equation to get:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{Var}(x_t)=\frac{\sigma_{\omega}^2}{1-\phi^2}
\end{equation}\]</span>.</p>
<h1 id="log-likelihood">Log-Likelihood</h1>
<p>In this part, we will derive the log-likelihood of the AR(1) model,
which can be used to estimate the parameters in the MLE approach.</p>
<p>Firstly, we define the parameters: <span class="math inline">\(\theta
= (\delta,\phi,\sigma_{\omega}^2)\)</span>.</p>
<p>Based on the Bayesian theory:</p>
<p><span class="math display">\[\begin{equation}
P(x_t,x_{t-1},...,x_1;\theta) =
P(x_t|x_{t-1},x_{t-2},...,x_1;\theta)P(x_{t-1}|x_{t-2},...,x_1;\theta)...P(x_2|x_1;\theta)P(x_1;\theta)
\end{equation}\]</span></p>
<p>As in the AR(1) model, we assume that the <span
class="math inline">\(x_t\)</span> only directly conditional on <span
class="math inline">\(x_{t-1}\)</span>, then the above equation can be
simplified as:</p>
<p><span class="math display">\[\begin{equation}
P(x_t,x_{t-1},...,x_1;\theta) =
P(x_t|x_{t-1};\theta)P(x_{t-1}|x_{t-2};\theta)...P(x_2|x_1;\theta)P(x_1;\theta)
\end{equation}\]</span></p>
<p>Then we can rewrite the log-likelihood as:</p>
<p><span class="math display">\[\begin{equation}
\ln{L(\theta|x)} =\sum_{t=2}^{T}{\ln{P(x_t|x_{t-1};\theta)}} +
\ln{P(x_1;\theta)}
\end{equation}\]</span></p>
<p>Secondly, consider the (weakly) stationary AR(1) model, we have:</p>
<p><span class="math display">\[\begin{equation}
x_t = \delta + \phi x_{t-1} + \omega_t
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\omega_t \backsim i.i.d.
N(0,\sigma_{\omega}^2)\)</span> and <span
class="math inline">\(|\phi|&lt;1\)</span>.</p>
<p>Based on this, we can rewrite it as:</p>
<p><span class="math display">\[\begin{equation}
x_t|x_{t-1} \backsim N(\delta +\phi x_{t-1},\sigma_{\omega}^2)
\end{equation}\]</span></p>
<p>which equals to:</p>
<p><span class="math display">\[\begin{equation}
P(x_t|x_{t-1};\theta) =
\frac{1}{\sqrt{2\pi}\sigma_{\omega}}e^{-\frac{1}{2\sigma_{\omega}^2}(x_t-\delta-\phi
x_{t-1})^2}
\end{equation}\]</span></p>
<p>Then we can derive:</p>
<p><span class="math display">\[\begin{equation*}
\begin{aligned}
\sum_{t=2}^{T}{\ln{P(x_t|x_{t-1};\theta)}} &amp;=
\sum_{t=2}^{T}{-\frac{1}{2}\ln{2\pi
\sigma_{\omega}^2}-\frac{1}{2\sigma_{\omega}^2}(x_t-\delta-\phi
x_{t-1})^2} \\
&amp;=
-\frac{T-1}{2}\ln{2\pi}-\frac{T-1}{2}\ln{\sigma_{\omega}^2}-\frac{1}{2\sigma_{\omega}^2}\sum_{t=2}^{T}{(x_t-\delta-\phi
x_{t-1})^2}
\end{aligned}
\end{equation*}\]</span></p>
<p>Next, we have proven that for all <span
class="math inline">\(t\in{1,2,...,T}\)</span>, the expectation and
variance of <span class="math inline">\(x_t\)</span> is
respectively:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{E}(x_t)=\frac{\delta}{1-\phi}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\mathrm{Var}(x_t)=\frac{\sigma_{\omega}^2}{1-\phi^2}
\end{equation}\]</span></p>
<p>So for <span class="math inline">\(x_1\)</span>, we have:</p>
<p><span class="math display">\[\begin{equation}
x_1 \backsim N(\frac{\delta}{1-\phi},\frac{\sigma_{\omega}^2}{1-\phi^2})
\end{equation}\]</span></p>
<p>which equals to:</p>
<p><span class="math display">\[\begin{equation}
P(x_1;\theta) =
\frac{\sqrt{1-\phi^2}}{\sqrt{2\pi}\sigma_{\omega}}e^{-\frac{1-\phi^2}{2\sigma_{\omega}^2}(x_1-\frac{\delta}{1-\phi})^2}
\end{equation}\]</span></p>
<p>Then we can derive:</p>
<p><span class="math display">\[\begin{equation}
\ln{P(x_1;\theta)} =
-\frac{1}{2}\ln{2\pi}-\frac{1}{2}\ln{\frac{\delta^2}{1-\phi^2}}-\frac{1-\phi^2}{2\sigma_{\omega}^2}(x_1-\frac{\delta}{1-\phi})^2
\end{equation}\]</span></p>
<p>Finally we can have the log-likelihood as:</p>
<p><span class="math display">\[\begin{equation*}
\begin{aligned}
\ln{L(\theta|x)} &amp;= \sum_{t=2}^{T}{\ln{P(x_t|x_{t-1};\theta)}} +
\ln{P(x_1;\theta)}\\
&amp;= -\frac{T}{2}\ln{2\pi} - \frac{1}{2}
\ln{\frac{\sigma_{\omega}^2}{1-\phi^2}}-\frac{1-\phi^2}{2\sigma_{\omega}^2}(x_1-\frac{\delta}{1-\phi})^2-\frac{T-1}{2}\ln{\sigma_{\omega}^2}-\frac{1}{2\sigma_{\omega}^2}\sum_{t=2}^{T}{(x_t-\delta-\phi
x_{t-1})^2}
\end{aligned}
\end{equation*}\]</span></p>
<h1 id="mle-regression">MLE Regression</h1>
<p>We can learn the parameters based on <em>MLE</em> to do regression.
The following codes are based on
<code>scipy.optimize.minimize</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize <span class="keyword">as</span> mm</span><br><span class="line"></span><br><span class="line">theta_real = [<span class="number">5</span>, <span class="number">0.6</span>, <span class="number">1</span>]  <span class="comment"># parameters \theta is defined as [\delta, \phi, \sigma]</span></span><br><span class="line">expectation_real = theta_real[<span class="number">0</span>] / (<span class="number">1</span> - theta_real[<span class="number">1</span>])  <span class="comment"># expectation</span></span><br><span class="line">variance_real = theta_real[<span class="number">2</span>] ** <span class="number">2</span> / (<span class="number">1</span> - theta_real[<span class="number">1</span>] ** <span class="number">2</span>)  <span class="comment"># variance</span></span><br><span class="line"></span><br><span class="line">T = <span class="number">100</span>  <span class="comment"># number of time points</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># generate x_1</span></span><br><span class="line">x_1 = np.random.normal(expectation_real, variance_real)</span><br><span class="line"><span class="comment"># generate x_t, t=2,3,...,T</span></span><br><span class="line">data = [x_1]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T - <span class="number">1</span>):</span><br><span class="line">    x_next = theta_real[<span class="number">0</span>] + theta_real[<span class="number">1</span>] * data[-<span class="number">1</span>] + np.random.normal(<span class="number">0</span>, theta_real[<span class="number">2</span>] ** <span class="number">2</span>)</span><br><span class="line">    data.append(x_next)</span><br><span class="line"></span><br><span class="line">data = np.array(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss function (negative of log-likelihood)</span></span><br><span class="line"><span class="comment"># params: \theta = [\delta, \phi, \sigma]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">params</span>):</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(params) == <span class="number">3</span>, <span class="string">&quot;incorrect shape of parameters&quot;</span></span><br><span class="line">    delta, phi, sigma = params</span><br><span class="line"></span><br><span class="line">    x_t = data[<span class="number">1</span>:]</span><br><span class="line">    x_t_1 = data[:-<span class="number">1</span>]</span><br><span class="line">    sum_2_T = np.<span class="built_in">sum</span>((x_t - delta - phi * x_t_1) ** <span class="number">2</span>)</span><br><span class="line">    log_likelihood = - <span class="number">0.5</span> * T * np.log(<span class="number">2</span> * <span class="number">3.14</span>) \</span><br><span class="line">                     - <span class="number">0.5</span> * np.log(sigma ** <span class="number">2</span> / (<span class="number">1</span> - phi ** <span class="number">2</span>)) \</span><br><span class="line">                     - <span class="number">0.5</span> * (<span class="number">1</span> - phi ** <span class="number">2</span>) / (sigma ** <span class="number">2</span>) * ((x_1 - delta / (<span class="number">1</span> - phi)) ** <span class="number">2</span>) \</span><br><span class="line">                     - (T - <span class="number">1</span>) / <span class="number">2</span> * np.log(sigma ** <span class="number">2</span>) \</span><br><span class="line">                     - <span class="number">0.5</span> / (sigma ** <span class="number">2</span>) * sum_2_T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> -log_likelihood</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># optimize with scipy.optimize.minimize</span></span><br><span class="line">params_ini = np.random.rand(<span class="number">3</span>)  <span class="comment"># initialize params ([\delta, \phi, \sigma])</span></span><br><span class="line">params_bounds = ((<span class="number">0</span>, <span class="number">10</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, <span class="number">2</span>))  <span class="comment"># set some boundaries for the parameters</span></span><br><span class="line">result = mm(loss_function, params_ini, bounds=params_bounds, method=<span class="string">&#x27;Powell&#x27;</span>, tol=<span class="number">1e-10</span>)</span><br><span class="line">params_predicted = result.x</span><br><span class="line"></span><br><span class="line"><span class="comment"># show results</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Real parameters [\delta, \phi, \sigma]: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(theta_real))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicted parameters [\delta, \phi, \sigma]: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(params_predicted))</span><br></pre></td></tr></table></figure>
<p>One of the results of the above codes is:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Real parameters [\delta, \phi, \sigma]: [<span class="number">5</span>, <span class="number">0.6</span>, <span class="number">1</span>]</span><br><span class="line">Predicted parameters [\delta, \phi, \sigma]: [<span class="number">4.84142903</span> <span class="number">0.60249039</span> <span class="number">1.0670589</span>]</span><br></pre></td></tr></table></figure>
<h1 id="main-reference">Main Reference</h1>
<ul>
<li>STAT 510 Course (1.1 &amp; 1.2) of Eberly College of Science of
*PennState: <a href="https://online.stat.psu.edu/stat510/lesson/1"
class="uri">https://online.stat.psu.edu/stat510/lesson/1</a></li>
<li>Estimation of ARMA Models by Eric Zivot: <a
href="https://faculty.washington.edu/ezivot/econ584/notes/armaestimation.pdf"
class="uri">https://faculty.washington.edu/ezivot/econ584/notes/armaestimation.pdf</a></li>
</ul>
]]></content>
      <categories>
        <category>Probabilistic Machine Learning</category>
      </categories>
      <tags>
        <tag>Probability Theory</tag>
        <tag>Machine Learning</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Negative Binomial Distribution - Expectation, Variance, Likelihood and Regression</title>
    <url>/2022/5-ProbabilisticML-2-NB/</url>
    <content><![CDATA[<p>This blog aims to show the detailed derivation of the expectation,
variance, likelihood and regression of <em>Negative Binomial
Distribution</em>.</p>
<p>The Poisson distribution can be generalized by including a gamma
noise variable, then we can get the negative binomial distribution.</p>
<span id="more"></span>
<h1 id="probability-density-formula">Probability Density Formula</h1>
<p><span class="math display">\[\begin{equation}
P(Y=y_i;\mu,\alpha) =
\frac{\Gamma(y_i+\alpha^{-1})}{\Gamma(y_i+1)\Gamma(\alpha^{-1})}(\frac{\alpha^{-1}}{\alpha^{-1}\mu})^{\alpha^{-1}}(\frac{\mu}{\alpha^{-1}+\mu})^{y_i}
\end{equation}\]</span></p>
<p>where: <span class="math display">\[\begin{equation}
\mu_i=e^{x_i\beta}
\end{equation}\]</span></p>
<p>We can set <span class="math inline">\(n=\alpha^{-1}\)</span> and
<span
class="math inline">\(p=\frac{\alpha^{-1}}{\alpha^{-1}+\mu}\)</span>,
then the above formula can be rewritten as:</p>
<p><span class="math display">\[\begin{equation}
P(Y=y_i;n,p) =
\frac{\Gamma(y_i+n)}{\Gamma(y_i+1)\Gamma(n)}p^n(1-p)^{y_i}
\end{equation}\]</span></p>
<h1 id="expectation-and-variance">Expectation and Variance</h1>
<p>The expectation and the variance is:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{E}(Y) = \frac{n(1-p)}{p}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\mathrm{Var}(Y) = \frac{n(1-p)}{p^2}
\end{equation}\]</span></p>
<h2 id="expectation">Expectation</h2>
<p>To proof:</p>
<p><span class="math display">\[\begin{equation*}
\begin{aligned}
\mathrm{E}(Y) &amp; =
\sum_{y=0}^{\infty}{y\frac{(y+n-1)!}{(y)!(n-1)!}p^n(1-p)^{y}} \\
&amp;= \sum_{y=0}^{\infty}{\frac{(y+n-1)!}{(y-1)!(n-1)!}p^n(1-p)^y}\\
&amp;= \sum_{y=0}^{\infty}{\frac{(y+n-1)!}{y!n!}p^{n+1}(1-p)^{y-1}}\\
&amp;= \frac{n(1-p)}{p}
\sum_{y=1}^{\infty}{\frac{(y+n-1)!}{(y-1)!n!}p^{n+1}(1-p)^{y-1}} \\
&amp;= \frac{n(1-p)}{p}
\sum_{z=0}^{\infty}{\frac{((n+1)+z-1)!}{z!n!}p^{n+1}(1-p)^{z}}
\end{aligned}
\end{equation*}\]</span></p>
<p>As the integral sum of the negative binomial distribution is 1:</p>
<p><span class="math display">\[\begin{equation}
\sum_{z=0}^{\infty}{\frac{((n+1)+z-1)!}{z!n!}p^{n+1}(1-p)^{z}} = 1
\end{equation}\]</span></p>
<p>So, we can get:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{E}(Y) = \frac{n(1-p)}{p}
\end{equation}\]</span></p>
<h2 id="variance">Variance</h2>
<p>Before we compute the variance, we firstly compute <span
class="math inline">\(\mathrm{E}(Y(Y-1))\)</span>:</p>
<p><span class="math display">\[\begin{equation*}
\begin{aligned}
\mathrm{E}(Y(Y-1)) &amp; =
\sum_{y=0}^{\infty}{y(y-1)\frac{(y+n-1)!}{y!(n-1)!}p^n(1-p)^y} \\
&amp;= \frac{n(r+1)(1-p)^2}{p^2}
\sum_{y=2}^{\infty}{\frac{((n+2)+(y-2)-1)!}{(y-2)!(n+1)!}p^{n+2}(1-p)^{y-2}}
\\
&amp;= \frac{n(r+1)(1-p)^2}{p^2}
\sum_{z=0}^{\infty}{\frac{((n+2)+z-1)!}{z!(n+1)!}p^{n+2}(1-p)^{z}} \\
&amp;= \frac{n(r+1)(1-p)^2}{p^2}
\end{aligned}
\end{equation*}\]</span></p>
<p>Then we can get:</p>
<p><span class="math display">\[\begin{equation}
E(Y^2) = E(Y(Y-1)) + E(Y) = \frac{n(n+1)(1-p)^2 + n(1-p)p}{p^2}
\end{equation}\]</span></p>
<p>Finally, we can compute the variance as:</p>
<p><span class="math display">\[\begin{equation}
\mathrm{Var}(Y) = \mathrm{E}(Y^2) - (\mathrm{E}(Y))^2 =
\frac{n(1-p)}{p^2}
\end{equation}\]</span></p>
<h1 id="log-likelihood">Log-Likelihood</h1>
<p>The log-likelihood that to maximize in an MLE approach is:</p>
<p><span class="math display">\[\begin{equation}
\sum_{i=1}^{n}{\ln(\Gamma(y_i+\alpha^{-1})) - \ln(\Gamma(\alpha^{-1})) -
\ln(\Gamma(y_i+1)) - (\alpha^{-1} + y_i)\ln(1+\alpha \mu_i)
+y_i(\ln(\alpha)+\ln(\mu_i))}
\end{equation}\]</span></p>
<h1 id="negative-binomial-regression">Negative Binomial Regression</h1>
<p>We can learn the parameters based on <em>MLE</em> to do regression.
The following codes are based on
<code>scipy.optimize.minimize</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize <span class="keyword">as</span> mm</span><br><span class="line"><span class="keyword">from</span> scipy.special <span class="keyword">import</span> loggamma</span><br><span class="line"></span><br><span class="line">N = <span class="number">1000</span>  <span class="comment"># number of data points</span></span><br><span class="line">P = <span class="number">5</span>  <span class="comment"># number of features</span></span><br><span class="line"></span><br><span class="line">X = np.random.randn(N, P)  <span class="comment"># matrix X in shape of NxP</span></span><br><span class="line">alpha_real = np.random.randint(<span class="number">1</span>, <span class="number">10</span>)  <span class="comment"># the real values of \alpha, a scalar</span></span><br><span class="line">beta_real = np.random.randn(P)  <span class="comment"># the real values of \beta, in shape of P</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To apply the np.random.negative_binomial function:</span></span><br><span class="line"><span class="comment"># n = 1 / alpha_real</span></span><br><span class="line"><span class="comment"># p = (1 / alpha_real) / ((1 / alpha_real) + exp(X * real_beta))</span></span><br><span class="line"></span><br><span class="line">n = <span class="number">1</span> / alpha_real</span><br><span class="line">p = n / (n + np.exp(np.dot(X, beta_real)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># The negative binomial distribution defined from np.random.negative_binomial(n,p):</span></span><br><span class="line"><span class="comment"># P(y; n, p) = ((y+n-1)!)/(y!(n-1)!) * (p^n) * (1-p)^y</span></span><br><span class="line">Y = np.random.negative_binomial(n, p)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss and gradient function</span></span><br><span class="line"><span class="comment"># params = [alpha, beta]</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">params</span>):</span></span><br><span class="line">    alpha = params[<span class="number">0</span>]</span><br><span class="line">    beta = params[<span class="number">1</span>:]</span><br><span class="line">    mu = np.exp(np.dot(X, beta))</span><br><span class="line"></span><br><span class="line">    log_likelihood = loggamma(Y + <span class="number">1</span> / alpha).<span class="built_in">sum</span>() \</span><br><span class="line">                     - loggamma(alpha) * N \</span><br><span class="line">                     - loggamma(Y + <span class="number">1</span>).<span class="built_in">sum</span>() \</span><br><span class="line">                     - <span class="number">1</span> / alpha * (np.log(<span class="number">1</span> + alpha * mu).<span class="built_in">sum</span>()) \</span><br><span class="line">                     - np.dot(Y, np.log(<span class="number">1</span> + alpha * mu)) \</span><br><span class="line">                     + (Y * np.log(alpha)).<span class="built_in">sum</span>() \</span><br><span class="line">                     + np.dot(Y, np.log(mu))</span><br><span class="line">    <span class="keyword">return</span> -log_likelihood</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># optimize with scipy.optimize.minimize</span></span><br><span class="line">params_ini = np.ones(P + <span class="number">1</span>)  <span class="comment"># initialize params ([alpha, beta]) with zeros</span></span><br><span class="line">result = mm(loss_function, params_ini)</span><br><span class="line">params_predicted = result.x</span><br><span class="line"></span><br><span class="line"><span class="comment"># show results</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Real alpha: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(alpha_real))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Real beta: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(beta_real))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicted alpha: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(params_predicted[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicted beta: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(params_predicted[<span class="number">1</span>:]))</span><br></pre></td></tr></table></figure>
<p>The output of the above codes:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Real alpha: <span class="number">2</span></span><br><span class="line">Real beta: [ <span class="number">1.6794512</span>  -<span class="number">0.22624054</span>  <span class="number">1.15579192</span>  <span class="number">0.35920843</span>  <span class="number">0.09326148</span>]</span><br><span class="line">Predicted alpha: <span class="number">2.0811572484796637</span></span><br><span class="line">Predicted beta: [ <span class="number">1.69292056</span> -<span class="number">0.28083378</span>  <span class="number">1.18706627</span>  <span class="number">0.43136325</span>  <span class="number">0.1085059</span> ]</span><br></pre></td></tr></table></figure>
<h1 id="main-reference">Main Reference</h1>
<ul>
<li>Wikipedia of Negative binomial distribution: <a
href="https://en.wikipedia.org/wiki/Negative_binomial_distribution"
class="uri">https://en.wikipedia.org/wiki/Negative_binomial_distribution</a></li>
<li>Negative Binomial Distributio introduction: <a
href="http://www.math.ntu.edu.tw/~hchen/teaching/StatInference/notes/lecture16.pdf"
class="uri">http://www.math.ntu.edu.tw/~hchen/teaching/StatInference/notes/lecture16.pdf</a></li>
<li>Chapter 326 from NCSS Statistical Software: <a
href="https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Negative_Binomial_Regression.pdf"
class="uri">https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Negative_Binomial_Regression.pdf</a></li>
</ul>
]]></content>
      <categories>
        <category>Probabilistic Machine Learning</category>
      </categories>
      <tags>
        <tag>Probability Theory</tag>
        <tag>Machine Learning</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>[DQN] Human-level control through deep reinforcement learning</title>
    <url>/2022/9-RLPaper-MF-1-DQN/</url>
    <content><![CDATA[<blockquote>
<p>[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. 2013. Playing
Atari with Deep Reinforcement Learning. DOI:<a
href="https://doi.org/10.48550/arXiv.1312.5602"
class="uri">https://doi.org/10.48550/arXiv.1312.5602</a></p>
</blockquote>
<blockquote>
<p>[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu,
Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas
K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir
Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra,
Shane Legg, and Demis Hassabis. 2015. Human-level control through deep
reinforcement learning. Nature 518, 7540 (February 2015), 529–533.
DOI:<a href="https://doi.org/10.1038/nature14236"
class="uri">https://doi.org/10.1038/nature14236</a></p>
</blockquote>
<span id="more"></span>
<figure>
<img src="/images/RLPapers/Model-Free/DeepQLearning/1-1.png"
alt="Playing Atari with Deep Reinforcement Learning." />
<figcaption aria-hidden="true">Playing Atari with Deep Reinforcement
Learning.</figcaption>
</figure>
<figure>
<img src="/images/RLPapers/Model-Free/DeepQLearning/1-2.png"
alt="Human-level control through deep reinforcement learning." />
<figcaption aria-hidden="true">Human-level control through deep
reinforcement learning.</figcaption>
</figure>
<h1 id="overview">Overview</h1>
<p>The proposed <strong><em>Deep Q-Learning Network</em></strong>
(<strong><em>DQN</em></strong>) is the first algorithm to introduce
(deep) neural networks as function approximators (in other words,
non-linear function approximators) into reinforcement learning. DQN is a
<strong>model-free</strong>, <strong>off-policy</strong>,
<strong>(Q-)value based</strong> algorithm, which can be implemented
onto environments with continuous state space but only with discrete
action space. DQN can be trained <strong>end-to-end</strong>, which
means it could receive high-dimensional state's representations as
input. The paper evaluated the DQN on 7 Atari games and 49 Atari games
with the same hyper-parameters which shows the great generalization
ability of DQN.</p>
<h1 id="main-problems-to-solve">Main Problems to Solve</h1>
<p>To introduce deep neural networks (non-linear functions) as
approximators in reinforcement learning has to overcome some
problems:</p>
<ol type="1">
<li>Most deep learning algorithms (like classifications, regressions)
assume that the data samples are independent, however, in RL
environments, the samples in a sequence are <strong>highly
correlated</strong>.</li>
<li>Most deep learning algorithms require large amounts of hand-labelled
training data, however, RL algorithms need to learn from observations
and information directly from environment, e.g., the scalar reward
signals, which is frequently sparse, noisy and delayed.</li>
<li>Most deep learning methods assume they are learnt in a fixed
underlaying distribution, however, in RL, the <strong>data distribution
changes</strong> as the agents updates their behaviors during the
learning process.</li>
</ol>
<h1 id="main-innovations">Main Innovations</h1>
<p>The paper mainly proposed two techniques to overcome the problems:
(a) <strong>experience replay</strong> and (b) <strong>target network
with periodically update</strong>:</p>
<ol type="1">
<li>Experience replay: initialize a replay buffer and store the
transitions <span class="math inline">\(\{s_t,a_t,r_t,s_{t+1}\}\)</span>
in it, for each iteration, randomly draw samples from the replay buffer
and train the network using stochastic gradient descent (SGD).
<ul>
<li>Can break the correlations among samples so as to reduce the
variance</li>
<li>Each sample can be drawn in many rounds of updates, which allows for
greater data efficiency.</li>
</ul></li>
<li>Target network with periodically update: the algorithm initializes
two networks with the same hyper-parameters and architecture, for each
iteration, update the policy network with SGD, for each <span
class="math inline">\(K\)</span> (a hyper-parameter) iterations,
directly copy the parameters in policy network to target network. In the
training, the action is selected by target network.
<ul>
<li>The distribution of the behavior to select the action can be
smooth.</li>
</ul></li>
</ol>
<p>The algorithm:</p>
<figure>
<img src="/images/RLPapers/Model-Free/DeepQLearning/1-3.png"
alt="DQN algorithm." />
<figcaption aria-hidden="true">DQN algorithm.</figcaption>
</figure>
<h1 id="implementation-details">Implementation Details</h1>
<ul>
<li>Using an adaptive learning rate method such as RMSProp or ADADELTA
that maintains a per-parameter learning rate α, and adjusts α according
to the history of gradient updates to that parameter.</li>
<li>Using the <strong>last 4 frames</strong> of a history and stacks
them as the input to the Q-network, which can capture the information
about objects' speed and acceleration.</li>
<li>Pre-processing to change the RGB screens to gray-scale images and
crop the central part.</li>
<li>During the training, the agent observes the environment and selects
an action for each <span class="math inline">\(k\)</span> frames of the
video game.</li>
</ul>
]]></content>
      <categories>
        <category>RL Paper Notes</category>
      </categories>
      <tags>
        <tag>Reinforcement Learning</tag>
        <tag>Paper Notes</tag>
        <tag>Model-Free RL</tag>
        <tag>Deep Q-Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Markov Chain Monte Carlo (MCMC) Estimation of AR(1) Model</title>
    <url>/2022/7-ProbabilisticML-4-MCMCAR1/</url>
    <content><![CDATA[<p>In the AR(1) model, we have:</p>
<p><span class="math display">\[\begin{equation}
x_t = \delta + \phi x_{t-1} + \omega_t
\end{equation}\]</span></p>
<p>with the following assumptions:</p>
<ul>
<li><span class="math inline">\(\omega_t \backsim
N(0,\sigma_{\omega}^2)\)</span> i.i.d..</li>
<li>Properties of the errors <span
class="math inline">\(\omega_t\)</span> are independent of <span
class="math inline">\(x_t\)</span>.</li>
<li>The series <span class="math inline">\(x_t\)</span> is (weakly)
stationary, which requires <span class="math inline">\(|\phi|\ &lt;
1\)</span>.</li>
</ul>
<p>Now, to simplify, we set <span
class="math inline">\(\sigma_{\omega}^2\)</span> to a constant value
<span class="math inline">\(1\)</span>. In the following sections, we
estimates the parameters <span class="math inline">\(\theta =
(\delta,\phi)\)</span> by Markov Chain Monte Carlo (MCMC) method.</p>
<span id="more"></span>
<h1 id="derivation-of-posteriors-of-parameters">Derivation of Posteriors
of Parameters</h1>
<p>We first derive the posteriors of <span
class="math inline">\(P(\delta | \phi, x)\)</span> and <span
class="math inline">\(P(\phi | \delta, x)\)</span>.</p>
<p>Based on the Bayes' rule, we have:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
P(\delta | \phi, x) &amp;= \frac{P(\delta, \phi, x)}{P(\phi,x)} \\
&amp;=\frac{P(x|\delta, \phi)P(\delta | \phi)P(\phi)}{P(x|\phi)P(\phi)}
\\
&amp;\propto P(x|\delta, \phi)P(\delta)
\end{aligned}
\end{equation}\]</span></p>
<p>The same as parameter <span class="math inline">\(\phi\)</span>, so
we can get:</p>
<p><span class="math display">\[\begin{equation}
P(\phi | \delta, x) \propto P(x|\delta, \phi)P(\phi)
\end{equation}\]</span></p>
<p>Based on the last section, we have derived that:</p>
<p><span class="math display">\[\begin{equation}
P(x|\theta) = P(x|\delta, \phi) =
P(x_1|\theta)\prod_{t=2}^{T}{P(x_t|x_{t-1},\theta)}
\end{equation}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{equation}
x_1\backsim N(\frac{\delta}{1-\phi},\frac{1}{1-\phi^2})
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
x_t\backsim N(\delta+\phi x_{t-1},1)
\end{equation}\]</span></p>
<p>Based on this, we have:</p>
<p><span class="math display">\[\begin{equation}
P(x|\delta, \phi) =
\frac{\sqrt{1-\phi^2}}{\sqrt{2\pi}}e^{-\frac{1-\phi^2}{2}(x_1-\frac{\delta}{1-\phi})^2}
\prod_{t=2}^{T}{\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_t-\delta-\phi
x_{t-1})^2}}
\end{equation}\]</span></p>
<p>To set (non-informative) priors of the two parameters:</p>
<p><span class="math display">\[\begin{equation}
P(\delta) \propto 1, \alpha \in \mathcal{R}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
P(\phi) = \frac{1}{2}, -1&lt;\phi &lt;1
\end{equation}\]</span></p>
<p>Or we can set the priors with normal distributions:</p>
<p><span class="math display">\[\begin{equation}
P(\delta)\backsim N(1,2)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
P(\phi)\backsim N(0.5,1)
\end{equation}\]</span></p>
<h1 id="acceptance-ratio">Acceptance Ratio</h1>
<p>In a Metropolis-Hastings algorithm, the acceptance ratio controls
whether to accept the proposed new sample <span
class="math inline">\(\theta_t\)</span> conditioned on <span
class="math inline">\(\theta_{t-1}\)</span>, which is defined as:</p>
<p><span class="math display">\[\begin{equation}
r=min \left[1,\frac{P(\theta_i|x)}{P(\theta_{i-1}|x)}\right] = min
\left[1,\frac{P(x|\theta_i)P(\theta_i)}{P(x|\theta_{i-1})P(\theta_{i-1})}\right]
\end{equation}\]</span></p>
<p>So, the acceptance ratios of the two parameters <span
class="math inline">\(\delta\)</span> and <span
class="math inline">\(\phi\)</span> are respectively listed below:</p>
<p><span class="math display">\[\begin{equation}
r_{\delta}=min
\left[1,\frac{P(\delta_i|\phi_{i-1},x)}{P(\delta_{i-1}|\phi_{i-1},x)}\right]
= min
\left[1,\frac{P(x|\delta_i,\phi_{i-1})P(\delta_i)}{P(x|\delta_{i-1},\phi_{i-1})P(\delta_{i-1})}\right]
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
r_{\phi}=min
\left[1,\frac{P(\phi_i|\delta_{i-1},x)}{P(\phi_{i-1}|\delta_{i-1},x)}\right]
= min
\left[1,\frac{P(x|\phi_i,\delta_{i-1})P(\phi_i)}{P(x|\phi_{i-1},\delta_{i-1})P(\phi_{i-1})}\right]
\end{equation}\]</span></p>
<p>Finally, in order to avoid the floating errors caused by multiplying
many decimals, we can translate the above acceptance ratios into
logarithms:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\log{r_{\delta}} &amp;=min
[0,\log{P(\delta_i|\phi_{i-1},x)}-\log{P(\delta_{i-1}|\phi_{i-1},x)}] \\
&amp;=min
[0,\log{P(x|\delta_i,\phi_{i-1})}+\log{P(\delta_i)}-\log{P(x|\delta_{i-1},\phi_{i-1})}-\log{P(\delta_{i-1})}]
\end{aligned}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
\log{r_{\phi}} &amp;=min
[0,\log{P(\phi_i|\delta_{i-1},x)}-\log{P(\phi_{i-1}|\delta_{i-1},x)}] \\
&amp;=min
[0,\log{P(x|\delta_{i-1},\phi_i)}+\log{P(\phi_i)}-\log{P(x|\delta_{i-1},\phi_{i-1})}-\log{P(\phi_{i-1})}]
\end{aligned}
\end{equation}\]</span></p>
<h1 id="mcmc-method-to-estimate-parameters">MCMC Method to Estimate
Parameters</h1>
<p>Now, suppose we have:</p>
<ol type="1">
<li>A time series <span class="math inline">\(\{x_t\}\)</span>;</li>
<li>Pre-defined priors of parameters: <span
class="math inline">\(P(\delta)\)</span> and <span
class="math inline">\(P(\phi)\)</span>;</li>
<li>Two start points for two parameters respectively: <span
class="math inline">\(\delta_0\)</span> and <span
class="math inline">\(\phi_0\)</span>;</li>
<li>Generative distributions of next sample given the current sample:
<span class="math inline">\(g(\delta_i | \delta_{i-1}) := \delta_i
\backsim N(\delta_{i-1},\sigma_{\delta}^2)\)</span> and <span
class="math inline">\(g(\phi_i | \phi_{i-1}) := \phi_i \backsim
N(\phi_{i-1},\sigma_{\phi}^2)\)</span>;</li>
<li>Pre-defined hyper-parameters, like: burn-in steps <span
class="math inline">\(B\)</span>, total numbers of samples to generate
<span class="math inline">\(N\)</span>.</li>
</ol>
<p>The main process of the Metropolis-Hastings algorithm to estimate the
parameters consists of the following steps:</p>
<ol type="1">
<li>Given the current samples <span
class="math inline">\(\delta_i\)</span> and <span
class="math inline">\(\phi_i\)</span>, generate samples for next step
<span class="math inline">\(\delta_{i+1}\)</span> and <span
class="math inline">\(\phi_{i+1}\)</span> based on the <span
class="math inline">\(g(\delta_i | \delta_{i-1})\)</span> and <span
class="math inline">\(g(\phi_i | \phi_{i-1})\)</span>.</li>
<li>Uniformly and randomly sample a <span class="math inline">\(u\in
[0,1]\)</span>, compute the ratio <span class="math inline">\(r\)</span>
and compare <span class="math inline">\(u\)</span> with <span
class="math inline">\(r\)</span> to determine whether to accept the new
samples.</li>
<li>Add the new samples is accepted or keep the current samples as new
samples if rejected.</li>
</ol>
<h1 id="codes">Codes</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Using Markov Chain Monte Carlo (MCMC) method to estimate parameters of AR(1) model.</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"></span><br><span class="line"><span class="comment"># ! Set hyper-parameters:</span></span><br><span class="line">theta_real = [<span class="number">0.3</span>, <span class="number">0.7</span>]  <span class="comment"># parameters \theta is defined as [\delta, \phi]</span></span><br><span class="line">sigma = <span class="number">1</span>  <span class="comment"># set \sigma as a constant value</span></span><br><span class="line"></span><br><span class="line">T = <span class="number">10000</span></span><br><span class="line">N = <span class="number">10200</span></span><br><span class="line">B = <span class="number">200</span></span><br><span class="line">x_0 = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">priors = &#123;<span class="string">&#x27;delta_E&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;delta_Var&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">          <span class="string">&#x27;phi_E&#x27;</span>: <span class="number">0.5</span>, <span class="string">&#x27;phi_Var&#x27;</span>: <span class="number">1</span>&#125;  <span class="comment"># to define the mean and variance of prior distributions of the parameters</span></span><br><span class="line"></span><br><span class="line">generate_sigma_square = <span class="number">0.5</span>  <span class="comment"># the variance to sample next parameter in a Normal method</span></span><br><span class="line">generate_e = <span class="number">0.2</span>  <span class="comment"># the upper/lower float to sample next parameter in a Uniform method</span></span><br><span class="line">generate_method = <span class="string">&quot;normal&quot;</span></span><br><span class="line"></span><br><span class="line">theta_0 = [<span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prior_theta</span>(<span class="params">theta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to define the priors of parameters (log).</span></span><br><span class="line"><span class="string">    :param theta: parameters [\delta, \phi]</span></span><br><span class="line"><span class="string">    :return: the prior probabilities.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    p_delta = norm.pdf(theta[<span class="number">0</span>], loc=priors[<span class="string">&#x27;delta_E&#x27;</span>], scale=priors[<span class="string">&#x27;delta_Var&#x27;</span>])</span><br><span class="line">    p_phi = norm.pdf(theta[<span class="number">1</span>], loc=priors[<span class="string">&#x27;phi_E&#x27;</span>], scale=priors[<span class="string">&#x27;phi_Var&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [np.log(p_delta), np.log(p_phi)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_time_series</span>(<span class="params">T, x_0</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function generates the time series given the length T and the first element.</span></span><br><span class="line"><span class="string">    :param T: the length of time series</span></span><br><span class="line"><span class="string">    :param x_0: the first element</span></span><br><span class="line"><span class="string">    :return: a time series, x.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = [x_0]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T - <span class="number">1</span>):</span><br><span class="line">        x_t = theta_real[<span class="number">0</span>] + theta_real[<span class="number">1</span>] * x[i] + np.random.normal(<span class="number">0</span>, sigma)</span><br><span class="line">        x.append(x_t)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_next</span>(<span class="params">theta_current, method=generate_method</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to generate the next sample.</span></span><br><span class="line"><span class="string">    :param theta_current: the current theta</span></span><br><span class="line"><span class="string">    :param method: can choose &quot;normal&quot; or &quot;uniform&quot;</span></span><br><span class="line"><span class="string">    :return: the next proposals of theta</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> method <span class="keyword">in</span> [<span class="string">&quot;normal&quot;</span>, <span class="string">&quot;uniform&quot;</span>], <span class="string">&quot;Unknown generative method!&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">&quot;normal&quot;</span>:</span><br><span class="line">        delta_proposal = np.random.normal(theta_current[<span class="number">0</span>], generate_sigma_square)</span><br><span class="line">        phi_proposal = np.random.normal(theta_current[<span class="number">1</span>], generate_sigma_square)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        delta_proposal = np.random.uniform(theta_current[<span class="number">0</span>] - generate_e, theta_current[<span class="number">0</span>] + generate_e)</span><br><span class="line">        phi_proposal = np.random.uniform(theta_current[<span class="number">1</span>] - generate_e, theta_current[<span class="number">1</span>] + generate_e)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [delta_proposal, phi_proposal]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_log_P_x_given_theta</span>(<span class="params">x_series, theta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to compute log(P(x|theta)).</span></span><br><span class="line"><span class="string">    The formula: log(P(x|theta)) = 0.5 * log(1 - \phi ^ 2) - 0.5 * (1 - \phi ^ 2) * (x_1 - \delta / (1 - \phi)) ^ 2</span></span><br><span class="line"><span class="string">        - 0.5 * SUM_&#123;t=2&#125;^&#123;T&#125;&#123;(x_t - \delta -\phi x_&#123;t-1&#125;) ^ 2&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param x_series: the time series, x.</span></span><br><span class="line"><span class="string">    :param theta: the parameters [delta_i, phi_j]</span></span><br><span class="line"><span class="string">    :return: the log(P(x|theta))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    x_1 = x_series[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    sum_term = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, T):</span><br><span class="line">        sum_term += (x_series[t] - theta[<span class="number">0</span>] - theta[<span class="number">1</span>] * x_series[t - <span class="number">1</span>]) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># to restrain the |\phi|&lt;1</span></span><br><span class="line">    <span class="keyword">if</span> theta[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        theta[<span class="number">1</span>] = <span class="number">0.99</span></span><br><span class="line">    <span class="keyword">if</span> theta[<span class="number">1</span>] &lt; -<span class="number">1</span>:</span><br><span class="line">        theta[<span class="number">1</span>] = -<span class="number">0.99</span></span><br><span class="line"></span><br><span class="line">    log_P = <span class="number">0.5</span> * np.log(<span class="number">1</span> - theta[<span class="number">1</span>] ** <span class="number">2</span>) \</span><br><span class="line">            - <span class="number">0.5</span> * (<span class="number">1</span> - theta[<span class="number">1</span>] ** <span class="number">2</span>) * (x_1 - theta[<span class="number">0</span>] / (<span class="number">1</span> - theta[<span class="number">1</span>])) ** <span class="number">2</span> \</span><br><span class="line">            - <span class="number">0.5</span> * sum_term</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> log_P</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept_or_reject_proposals</span>(<span class="params">x_series, theta_current, theta_proposal</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to determine the theta_next by accepting or rejecting the proposals</span></span><br><span class="line"><span class="string">    :param x_series: the time series, x</span></span><br><span class="line"><span class="string">    :param theta_current: the current parameters [\delta_i, \phi_i]</span></span><br><span class="line"><span class="string">    :param theta_proposal: the proposed parameters [\delta_p, \phi_p]</span></span><br><span class="line"><span class="string">    :return: the new theta [\delta_&#123;i+1&#125;, \phi_&#123;i+1&#125;], the [signals] of accepting (for True) or rejecting (for False)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    accept = [<span class="literal">False</span>, <span class="literal">False</span>]</span><br><span class="line">    theta_next = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    log_p_x_theta_i = compute_log_P_x_given_theta(x_series, theta_current)</span><br><span class="line">    log_theta_i = prior_theta(theta_current)</span><br><span class="line">    log_theta_p = prior_theta(theta_proposal)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># + For parameter \delta, to compare log(P(\delta_p | \phi_i, x)) and log(P(\delta_i | \phi_i, x)), which equals to:</span></span><br><span class="line">    <span class="comment"># + log(r) = log(P(x | \delta_p, \phi_i)) + log(P(\delta_p)) - log(P(x | \delta_i, \phi_i)) - log(P(\delta_i))</span></span><br><span class="line">    log_r_delta = compute_log_P_x_given_theta(x_series, [theta_proposal[<span class="number">0</span>], theta_current[<span class="number">1</span>]]) \</span><br><span class="line">                  + log_theta_p[<span class="number">0</span>] \</span><br><span class="line">                  - log_p_x_theta_i \</span><br><span class="line">                  - log_theta_i[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> log_r_delta &gt; <span class="number">0</span>:</span><br><span class="line">        accept[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">        theta_next[<span class="number">0</span>] = theta_proposal[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        log_u = np.log(np.random.uniform(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> log_u &lt; log_r_delta:</span><br><span class="line">            accept[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">            theta_next[<span class="number">0</span>] = theta_proposal[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># ! reject</span></span><br><span class="line">            accept[<span class="number">0</span>] = <span class="literal">False</span></span><br><span class="line">            theta_next[<span class="number">0</span>] = theta_current[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># + For parameter \phi, to compare log(P(\phi_p | /delta_i, x)) and log(P(\phi_i | \delta_i, x)), which equals to:</span></span><br><span class="line">    <span class="comment"># + log(r) = log(P(x | \delta_i, \phi_p)) + log(P(\phi_p)) - log(P(x | \delta_i, \phi_i)) - log(P(\phi_i))</span></span><br><span class="line">    log_r_phi = compute_log_P_x_given_theta(x_series, [theta_current[<span class="number">0</span>], theta_proposal[<span class="number">1</span>]]) \</span><br><span class="line">                + log_theta_p[<span class="number">1</span>] \</span><br><span class="line">                - log_p_x_theta_i \</span><br><span class="line">                - log_theta_i[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> log_r_phi &gt; <span class="number">0</span>:</span><br><span class="line">        accept[<span class="number">1</span>] = <span class="literal">True</span></span><br><span class="line">        theta_next[<span class="number">1</span>] = theta_proposal[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        log_u = np.log(np.random.uniform(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> log_u &lt; log_r_phi:</span><br><span class="line">            accept[<span class="number">1</span>] = <span class="literal">True</span></span><br><span class="line">            theta_next[<span class="number">1</span>] = theta_proposal[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># ! reject</span></span><br><span class="line">            accept[<span class="number">1</span>] = <span class="literal">False</span></span><br><span class="line">            theta_next[<span class="number">1</span>] = theta_current[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta_next, accept</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perform_mcmc</span>(<span class="params">N, B, x_series, theta_0</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to perform MCMC sampling.</span></span><br><span class="line"><span class="string">    :param N: total number of samples</span></span><br><span class="line"><span class="string">    :param B: steps for burn-in</span></span><br><span class="line"><span class="string">    :param x_series: the time series, x</span></span><br><span class="line"><span class="string">    :param theta_0: the starting point</span></span><br><span class="line"><span class="string">    :return: the samples</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    samples = [theta_0]</span><br><span class="line">    accept_records = [[<span class="literal">True</span>, <span class="literal">True</span>]]  <span class="comment"># to record the acceptance</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, N):</span><br><span class="line">        theta_proposal = generate_next(samples[n - <span class="number">1</span>])</span><br><span class="line">        theta_next, accept = accept_or_reject_proposals(x_series, samples[n - <span class="number">1</span>], theta_proposal)</span><br><span class="line">        samples.append(theta_next)</span><br><span class="line">        accept_records.append(accept)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> samples, accept_records</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_figures</span>(<span class="params">samples, means</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to plot the samples.</span></span><br><span class="line"><span class="string">    :param samples: the samples</span></span><br><span class="line"><span class="string">    :param means: the means of two sets of samples (without burn-in)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    delta_samples = samples[<span class="number">0</span>]</span><br><span class="line">    phi_samples = samples[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># draw \delta figure</span></span><br><span class="line">    plt.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(N)), delta_samples, label=<span class="string">&quot;samples&quot;</span>)</span><br><span class="line">    plt.vlines(B, <span class="built_in">min</span>(delta_samples), <span class="built_in">max</span>(delta_samples), colors=<span class="string">&quot;r&quot;</span>, label=<span class="string">&quot;burn-in&quot;</span>)</span><br><span class="line">    plt.hlines(means[<span class="number">0</span>], <span class="number">0</span>, N, colors=<span class="string">&quot;m&quot;</span>, label=<span class="string">&quot;mean of samples&quot;</span>)</span><br><span class="line">    plt.hlines(theta_real[<span class="number">0</span>], <span class="number">0</span>, N, colors=<span class="string">&quot;g&quot;</span>, label=<span class="string">&quot;real value&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(<span class="string">r&quot;Samples of \delta&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># draw \phi figure</span></span><br><span class="line">    plt.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(N)), phi_samples, label=<span class="string">&quot;samples&quot;</span>)</span><br><span class="line">    plt.vlines(B, <span class="built_in">min</span>(phi_samples), <span class="built_in">max</span>(phi_samples), colors=<span class="string">&quot;r&quot;</span>, label=<span class="string">&quot;burn-in&quot;</span>)</span><br><span class="line">    plt.hlines(means[<span class="number">1</span>], <span class="number">0</span>, N, colors=<span class="string">&quot;m&quot;</span>, label=<span class="string">&quot;mean of samples&quot;</span>)</span><br><span class="line">    plt.hlines(theta_real[<span class="number">1</span>], <span class="number">0</span>, N, colors=<span class="string">&quot;g&quot;</span>, label=<span class="string">&quot;real value&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(<span class="string">r&quot;Samples of \phi&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    x_series = generate_time_series(T, x_0)</span><br><span class="line">    samples, accept_records = perform_mcmc(N, B, x_series, theta_0)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># To analyse the results</span></span><br><span class="line">    samples = np.array(samples).T</span><br><span class="line">    samples_without_burn_in = samples[:, B:]</span><br><span class="line">    means = np.mean(samples_without_burn_in, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    plot_figures(samples, means)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Real values of theta (delta, phi): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(theta_real))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Estimated values of theta (delta, phi): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(means))</span><br></pre></td></tr></table></figure>
<p>The results of one experiment:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Real values of theta (delta, phi): [<span class="number">0.3</span>, <span class="number">0.7</span>]</span><br><span class="line">Estimated values of theta (delta, phi): [<span class="number">0.30263395</span> <span class="number">0.69404937</span>]</span><br></pre></td></tr></table></figure>
<p>The following figures show the sample results of the above
experiment:</p>
<p><img src="/images/C7-F1.png"
alt="Samples generated in MCMC experiment of parameter \delta" /> <img
src="/images/C7-F2.png"
alt="Samples generated in MCMC experiment of parameter \phi" /></p>
<h1 id="main-reference">Main Reference</h1>
<p>[1] Kar Wai Lim. 2011. Bayesian analysis of claim run-off triangles.
(2011). DOI:<a href="https://doi.org/10.25911/5d778abf64951"
class="uri">https://doi.org/10.25911/5d778abf64951</a></p>
<p>[2] Carlo Parodi. 2020. An AR(1) model estimation with Metropolis
Hastings algorithm. Analytics Vidhya. Retrieved February 11, 2022 from
<a
href="https://medium.com/analytics-vidhya/an-ar-1-model-estimation-with-metropolis-hastings-algorithm-a5a0fcc0675f"
class="uri">https://medium.com/analytics-vidhya/an-ar-1-model-estimation-with-metropolis-hastings-algorithm-a5a0fcc0675f</a></p>
]]></content>
      <categories>
        <category>Probabilistic Machine Learning</category>
      </categories>
      <tags>
        <tag>Probability Theory</tag>
        <tag>Machine Learning</tag>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>Markov Chain Monte Carlo (MCMC) Estimation of Hidden Markov Model (HMM)</title>
    <url>/2022/8-ProbabilisticML-5-MCMCHMM/</url>
    <content><![CDATA[<p><strong>Hidden Markov Model</strong> (HMM) contains a sequence hidden
variables <span class="math inline">\(\{X_1, X_2, ..., X_T\}\)</span>
and a sequence of observable variables <span
class="math inline">\(\{Y_1, Y_2, ..., Y_T\}\)</span>, shown in the
following figure. The joint probability of an HMM is:</p>
<p><span class="math display">\[\begin{equation}
P(X_1, X_2, ..., X_T, Y_1, Y_2, ..., Y_T) =
P(X_1)\prod_{t=2}^{T}{P(X_t|X_{t-1})}\prod_{t=1}^{T}{P(Y_t|X_t)}
\end{equation}\]</span>.</p>
<span id="more"></span>
<figure>
<img src="/images/C8-F1.png" alt="Hidden Markov Model (HMM)." />
<figcaption aria-hidden="true">Hidden Markov Model (HMM).</figcaption>
</figure>
<p>This section derives how to use MCMC algorithm to learn parameters in
HMM. Let's suppose our ground truth is as follows:</p>
<ul>
<li>The hidden variables are generated based on AR(1) model, which
means:
<ul>
<li><span class="math inline">\(x_1\backsim
N(\frac{\delta}{1-\phi},\frac{\sigma_{\omega}^2}{1-\phi^2})\)</span></li>
<li><span class="math inline">\(x_t\backsim N(\delta+\phi
x_{t-1},\sigma_{\omega}^2)\)</span></li>
</ul></li>
<li>The observable variables are generated based on Normal distribution
averaged by the hidden variables, which means: <span
class="math inline">\(Y_t \backsim N(AX_t, \sigma_y^2)\)</span>.</li>
<li>To simplify, we fix <span class="math inline">\(\sigma_{\omega}^2 =
\sigma_y^2 = 1\)</span>.</li>
<li>The parameters we are going to learn are defined as <span
class="math inline">\(\theta = \{\delta, \phi, A\}\)</span>.
\end{itemize}</li>
</ul>
<h1 id="casual-inference-model-and-independence">Casual Inference Model
and Independence</h1>
<p>Based on the above assumptions, we can draw a casual inference model
for the variables shown in the following figure:</p>
<figure>
<img src="/images/C8-F2.png" alt="Hidden Markov Model (HMM)." />
<figcaption aria-hidden="true">Hidden Markov Model (HMM).</figcaption>
</figure>
<p>Based on the relations in the figure and the algorithm, we can
briefly derive the following independence:</p>
<ul>
<li><span class="math inline">\(\delta \perp Y, A | X\)</span>.</li>
<li><span class="math inline">\(\phi \perp Y, A | X\)</span>.</li>
<li><span class="math inline">\(A \perp \delta, \phi | X\)</span>.</li>
<li><span class="math inline">\(Y \perp \delta, \phi | X\)</span>.</li>
</ul>
<h1 id="posteriors-derivations-in-mcmc-algorithm">Posteriors Derivations
in MCMC Algorithm</h1>
<p>In this section, we derive the posteriors for the parameters we want
to estimate in MCMC algorithm.</p>
<p>Before we start, let's set the prior distributions for the
parameters:</p>
<p><span class="math display">\[\begin{equation}
\delta \backsim N(1,1)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\phi \backsim N(0.5,1)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
A \backsim N(2,1)
\end{equation}\]</span></p>
<p>Firstly, for <span class="math inline">\(P(\delta | \phi, X, Y,
A)\)</span>, based on the independence and Bayes' rule, we can
derive:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
P(\delta | \phi, X, Y, A) &amp;= P(\delta | \phi, X) \\
&amp;= \frac{P(\delta, \phi, X)}{P(\phi, X)} \\
&amp;=\frac{P(X|\delta, \phi)P(\delta | \phi)P(\phi)}{P(X|\phi)P(\phi)}
\\
&amp;\propto P(X|\delta, \phi)P(\delta)
\end{aligned}
\end{equation}\]</span></p>
<p>The same as parameter <span class="math inline">\(\phi\)</span> that
we can get:</p>
<p><span class="math display">\[\begin{equation}
P(\phi | \delta, X, Y, A) = P(\phi | \delta, X) \propto P(X|\delta,
\phi)P(\phi)
\end{equation}\]</span></p>
<p>Secondly, for <span class="math inline">\(P(A | \delta, \phi, X,
Y)\)</span>, we can derive:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
P(A | \delta, \phi, X, Y) &amp;= P(A | X, Y) \\
&amp;= \frac{P(Y | X, A)P(A|X)P(X)}{P(Y|X)P(X)} \\
&amp;\propto P(Y | X, A)P(A) \\
&amp;= p(A)\prod_{t=1}^{T}{P(Y_t | A, X_t)}
\end{aligned}
\end{equation}\]</span></p>
<p>Finally, we also need to derive the distributions for each hidden
variables <span class="math inline">\(X_t\)</span>:</p>
<ol type="A">
<li>For <span class="math inline">\(X_t\)</span>, where <span
class="math inline">\(t \neq 1, T\)</span>:</li>
</ol>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
&amp;P(X_t | X_1, ..., X_{t-2}, X_{t-1}, X_{t+1}, X_{t+2}, ..., X_T,
Y_1, Y_2, ..., Y_T, \delta, \phi, A) \\
&amp;= P(X_t | X_{t-1}, X_{t+1}, Y_t, \delta, \phi, A) \\
&amp;\propto P(X_{t-1}, X_t, X_{t+1}, Y_t, \delta, \phi, A) \\
&amp;= P(Y_t | X_t, A)P(X_{t+1} | X_t, \delta, \phi)P(X_t | X_{t-1},
\delta, \phi)P(X_{t-1} | \delta, \phi)P(A)P(\delta)P(\phi)\\
&amp;\propto P(Y_t | X_t, A)P(X_{t+1} | X_t, \delta, \phi)P(X_t |
X_{t-1}, \delta, \phi)
\end{aligned}
\end{equation}\]</span></p>
<p>Based on this, we can further derive that:</p>
<p><span class="math display">\[\begin{equation}
X_t | X_{t-1}, X_{t+1}, Y_t, \delta, \phi, A \backsim N(\mu_t,
\sigma_t^2)
\end{equation}\]</span></p>
<p>where: <span class="math inline">\(\mu_t = \frac{b_t}{a_t}\)</span>
and <span class="math inline">\(\sigma_t^2 = \frac{1}{a_t}\)</span>, and
we represent:</p>
<p><span class="math display">\[\begin{equation}
a_t = A^2 + \phi^2+1
\end{equation}\]</span> <span class="math display">\[\begin{equation}
b_t = AY_t+ \delta +\phi X_{t+1} + \phi X_{t-1} - \delta\phi
\end{equation}\]</span></p>
<ol start="2" type="A">
<li>For <span class="math inline">\(X_1\)</span> and <span
class="math inline">\(X_T\)</span>, we can similarly derive:</li>
</ol>
<p><span class="math display">\[\begin{equation}
P(X_1 | X_2, ..., X_T, Y_1, Y_2, ..., Y_T, \delta, \phi, A) \propto
P(Y_1 | X_1, A)P(X_2 | X_1, \delta, \phi)P(X_1 | \delta, \phi)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
P(X_T | X_1, x_2, ..., X_{T-1}, Y_1, Y_2, ..., Y_T, \delta, \phi, A)
\propto P(Y_T | X_T, A)P(X_T | X_{T-1}, \delta, \phi)
\end{equation}\]</span></p>
<p>We can further derive that:</p>
<p><span class="math display">\[\begin{equation}
X_1 | X_2, Y_1, \delta, \phi, A \backsim N(\mu_1, \sigma_1^2)
\end{equation}\]</span></p>
<p>where: <span class="math inline">\(\mu_1 = \frac{b_1}{a_1}\)</span>
and <span class="math inline">\(\sigma_1^2 = \frac{1}{a_1}\)</span>, and
we represent:</p>
<p><span class="math display">\[\begin{equation}
a_1 = A^2 + 1
\end{equation}\]</span> <span class="math display">\[\begin{equation}
b_1 = AY_1+\phi X_2 + \delta
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
X_T | X_{T-1}, Y_T, \delta, \phi, A \backsim N(\mu_T, \sigma_T^2)
\end{equation}\]</span></p>
<p>where: <span class="math inline">\(\mu_T = \frac{b_T}{a_T}\)</span>
and <span class="math inline">\(\sigma_T^2 = \frac{1}{a_T}\)</span>, and
we represent:</p>
<p><span class="math display">\[\begin{equation}
a_T = A^2 + 1
\end{equation}\]</span> <span class="math display">\[\begin{equation}
b_T = AY_T+\phi X_{T-1} + \delta
\end{equation}\]</span></p>
<p>Based on what we have known:</p>
<p><span class="math display">\[\begin{equation}
P(X_1|\delta,\phi) =
\frac{\sqrt{1-\phi^2}}{\sqrt{2\pi}}e^{-\frac{1-\phi^2}{2}(X_1-\frac{\delta}{1-\phi})^2}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
P(X_t | X_{t-1},\delta,\phi) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(X_t
- \delta - \phi X_{t-1})^2}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
P(X_1,X_2,...,X_t|\delta, \phi) =
\frac{\sqrt{1-\phi^2}}{\sqrt{2\pi}}e^{-\frac{1-\phi^2}{2}(X_1-\frac{\delta}{1-\phi})^2}
\prod_{t=2}^{T}{\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(X_t-\delta-\phi
X_{t-1})^2}}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
P(Y_t | X_t,A) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(Y_t - AX_t)^2}
\end{equation}\]</span></p>
<p>We can subsequent these equations into the results we have derived
above, we can compute the posteriors for the parameters and hidden
variables.</p>
<h1 id="acceptance-ratios">Acceptance Ratios</h1>
<p>To estimate the parameters using the Metropolis-Hastings algorithm,
we need to compute the acceptance ratio as follows:</p>
<p><span class="math display">\[\begin{equation}
r=min
\left[1,\frac{P(\theta_i|\theta_{i-1},X_{i-1},Y_{i-1})}{P(\theta_{i-1}|\theta_{i-1},X_{i-1},Y_{i-1}))}\right]
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> represents one of
the <span class="math inline">\(\{\delta, \phi, A\}\)</span> and the
conditioned <span class="math inline">\(\theta\)</span> represents other
two parameters.</p>
<p>In the implementation, we usually compute the <span
class="math inline">\(log\)</span> format to prevent over-floating.</p>
<h1 id="mcmc-algorithm-to-estimate-parameters">MCMC Algorithm to
Estimate Parameters</h1>
<p>Suppose we have a sequence of observed variables <span
class="math inline">\(\{Y_1, Y_2, ..., Y_T\}\)</span>, we initialize the
sequences for samples of the parameters: <span
class="math inline">\(\{\delta_0\}\)</span>, <span
class="math inline">\(\{\phi_0\}\)</span>, <span
class="math inline">\(\{A_0\}\)</span>. We also have the pre-defined
hyper-parameters, the burn-in steps <span
class="math inline">\(B\)</span> and the total steps to iterative
generate samples <span class="math inline">\(N\)</span>.</p>
<p>We also need to define the generating distributions to sample
parameters next step given the current parameters:</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
g(\delta_i | \delta_{i-1}) &amp;:= \delta_i \backsim N(\delta_{i-1},1)
\\
g(\phi_i | \phi_{i-1}) &amp;:= \phi_i \backsim N(\phi_{i-1},1) \\
g(A_i | A_{i-1}) &amp;:= A_i \backsim N(A_{i-1},1)
\end{aligned}
\end{equation}\]</span></p>
<p>The Metropolis-Hastings algorithm repeatedly does the following
steps:</p>
<ul>
<li>Sample a sequence of <span class="math inline">\(\{X_1, X_2, ...,
X_T\}_i\)</span>.</li>
<li>Generate candidate parameters <span
class="math inline">\(\{\hat{\delta}, \hat{\phi},
\hat{A}\}\)</span>.</li>
<li>Compute the acceptance ratio <span
class="math inline">\(r\)</span>.</li>
<li>Uniformly and randomly sample a <span class="math inline">\(u\in
[0,1]\)</span> and compare with <span class="math inline">\(r\)</span>
to determine whether to accept the new samples.</li>
<li>Add the new samples if accepted or keep the current samples as new
samples if rejected.</li>
</ul>
<h1 id="codes">Codes</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Using Markov Chain Monte Carlo (MCMC) method to estimate parameters of Hidden Markov Model (HMM).</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="comment"># ! Set hyper-parameters:</span></span><br><span class="line">theta_real = [<span class="number">0.3</span>, <span class="number">0.7</span>, <span class="number">2</span>]  <span class="comment"># parameters \theta is defined as [\delta, \phi, A]</span></span><br><span class="line">sigma = <span class="number">1</span>  <span class="comment"># set \sigma_\omega and \sigma_y as constant values (for all sigmas in the model)</span></span><br><span class="line"></span><br><span class="line">T = <span class="number">20000</span></span><br><span class="line">N = <span class="number">11000</span></span><br><span class="line">B = <span class="number">1000</span></span><br><span class="line">x_0 = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">priors = &#123;<span class="string">&#x27;delta_E&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;delta_Var&#x27;</span>: <span class="number">2</span>,</span><br><span class="line">          <span class="string">&#x27;phi_E&#x27;</span>: <span class="number">0.5</span>, <span class="string">&#x27;phi_Var&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">          <span class="string">&#x27;A_E&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;A_Var&#x27;</span>: <span class="number">2</span>&#125;  <span class="comment"># to define the mean and variance of prior distributions of the parameters</span></span><br><span class="line"></span><br><span class="line">generate_sigma_square = <span class="number">0.5</span>  <span class="comment"># the variance to sample next parameter in a Normal method</span></span><br><span class="line">generate_e = <span class="number">0.2</span>  <span class="comment"># the upper/lower float to sample next parameter in a Uniform method</span></span><br><span class="line">generate_method = <span class="string">&quot;normal&quot;</span></span><br><span class="line"></span><br><span class="line">x_series_0 = [<span class="number">1</span>] * T</span><br><span class="line">theta_0 = [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_series</span>(<span class="params">T, x_0</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    This function generates the series &#123;Y_1, Y_2, ..., Y_T&#125; given the length T and the first element X_0.</span></span><br><span class="line"><span class="string">    :param T: the length of time series</span></span><br><span class="line"><span class="string">    :param x_0: the first element of the hidden variables X_0</span></span><br><span class="line"><span class="string">    :return: a time series, &#123;Y_t&#125;.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x_t = x_0</span><br><span class="line">    y_0 = np.random.normal(theta_real[<span class="number">2</span>] * x_t, sigma)</span><br><span class="line">    Y = [y_0]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T - <span class="number">1</span>):</span><br><span class="line">        x_t = theta_real[<span class="number">0</span>] + theta_real[<span class="number">1</span>] * x_t + np.random.normal(<span class="number">0</span>, sigma)</span><br><span class="line">        y_t = np.random.normal(theta_real[<span class="number">2</span>] * x_t, sigma)</span><br><span class="line">        Y.append(y_t)</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prior_theta</span>(<span class="params">theta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to define the priors of parameters (log).</span></span><br><span class="line"><span class="string">    :param theta: parameters [\delta, \phi, A]</span></span><br><span class="line"><span class="string">    :return: the prior probabilities.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    p_delta = norm.pdf(theta[<span class="number">0</span>], loc=priors[<span class="string">&#x27;delta_E&#x27;</span>], scale=priors[<span class="string">&#x27;delta_Var&#x27;</span>])</span><br><span class="line">    p_phi = norm.pdf(theta[<span class="number">1</span>], loc=priors[<span class="string">&#x27;phi_E&#x27;</span>], scale=priors[<span class="string">&#x27;phi_Var&#x27;</span>])</span><br><span class="line">    p_A = norm.pdf(theta[<span class="number">2</span>], loc=priors[<span class="string">&#x27;A_E&#x27;</span>], scale=priors[<span class="string">&#x27;A_Var&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [np.log(p_delta), np.log(p_phi), np.log(p_A)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_next</span>(<span class="params">theta_current, method=generate_method</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to generate the next sample.</span></span><br><span class="line"><span class="string">    :param theta_current: the current theta</span></span><br><span class="line"><span class="string">    :param method: can choose &quot;normal&quot; or &quot;uniform&quot;</span></span><br><span class="line"><span class="string">    :return: the next proposals of theta</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> method <span class="keyword">in</span> [<span class="string">&quot;normal&quot;</span>, <span class="string">&quot;uniform&quot;</span>], <span class="string">&quot;Unknown generative method!&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">&quot;normal&quot;</span>:</span><br><span class="line">        delta_proposal = np.random.normal(theta_current[<span class="number">0</span>], generate_sigma_square)</span><br><span class="line">        phi_proposal = np.random.normal(theta_current[<span class="number">1</span>], generate_sigma_square)</span><br><span class="line">        A_proposal = np.random.normal(theta_current[<span class="number">2</span>], generate_sigma_square)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        delta_proposal = np.random.uniform(theta_current[<span class="number">0</span>] - generate_e, theta_current[<span class="number">0</span>] + generate_e)</span><br><span class="line">        phi_proposal = np.random.uniform(theta_current[<span class="number">1</span>] - generate_e, theta_current[<span class="number">1</span>] + generate_e)</span><br><span class="line">        A_proposal = np.random.uniform(theta_current[<span class="number">2</span>] - generate_e, theta_current[<span class="number">2</span>] + generate_e)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [delta_proposal, phi_proposal, A_proposal]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_x_series</span>(<span class="params">y_series, x_series, theta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to sample hidden variable series x_series given the observed variables y_series and current theta.</span></span><br><span class="line"><span class="string">    :param y_series: the observed variables y_series</span></span><br><span class="line"><span class="string">    :param x_series: the hidden variables x_series</span></span><br><span class="line"><span class="string">    :param theta: the parameters [delta_i, phi_i, A]</span></span><br><span class="line"><span class="string">    :return: the sampled x_series</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    x_series_new = []</span><br><span class="line"></span><br><span class="line">    delta, phi, A = theta</span><br><span class="line"></span><br><span class="line">    a_1 = A ** <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">    b_1 = A * y_series[<span class="number">0</span>] + phi * x_series[<span class="number">1</span>] + delta</span><br><span class="line">    mu_1 = b_1 / a_1</span><br><span class="line">    sigma_1 = <span class="number">1</span> / np.sqrt(a_1)</span><br><span class="line"></span><br><span class="line">    x_series_new.append(np.random.normal(mu_1, sigma_1))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, T - <span class="number">1</span>):</span><br><span class="line">        a_t = A ** <span class="number">2</span> + phi ** <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">        b_t = A * y_series[t] + delta + phi * (x_series[t + <span class="number">1</span>] + x_series[t - <span class="number">1</span>]) - delta * phi</span><br><span class="line">        mu_t = b_t / a_t</span><br><span class="line">        sigma_t = <span class="number">1</span> / np.sqrt(a_t)</span><br><span class="line">        x_series_new.append(np.random.normal(mu_t, sigma_t))</span><br><span class="line"></span><br><span class="line">    a_T = A ** <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">    b_T = A * y_series[T - <span class="number">1</span>] + phi * x_series[T - <span class="number">2</span>] + delta</span><br><span class="line">    mu_T = b_T / a_T</span><br><span class="line">    sigma_T = <span class="number">1</span> / np.sqrt(a_T)</span><br><span class="line"></span><br><span class="line">    x_series_new.append(np.random.normal(mu_T, sigma_T))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_series_new</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_log_P_x_given_theta</span>(<span class="params">x_series, theta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to compute log(P(x|theta)).</span></span><br><span class="line"><span class="string">    The formula: log(P(x|theta)) = 0.5 * log(1 - \phi ^ 2) - 0.5 * (1 - \phi ^ 2) * (x_1 - \delta / (1 - \phi)) ^ 2</span></span><br><span class="line"><span class="string">        - 0.5 * SUM_&#123;t=2&#125;^&#123;T&#125;&#123;(x_t - \delta -\phi x_&#123;t-1&#125;) ^ 2&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param x_series: the hidden variable series, x.</span></span><br><span class="line"><span class="string">    :param theta: the parameters [delta_i, phi_i, A_i]</span></span><br><span class="line"><span class="string">    :return: the log(P(x|delta_i, phi_i))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    x_1 = x_series[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    sum_term = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, T):</span><br><span class="line">        sum_term += (x_series[t] - theta[<span class="number">0</span>] - theta[<span class="number">1</span>] * x_series[t - <span class="number">1</span>]) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># to restrain the |\phi|&lt;1</span></span><br><span class="line">    <span class="keyword">if</span> theta[<span class="number">1</span>] &gt; <span class="number">1</span>:</span><br><span class="line">        theta[<span class="number">1</span>] = <span class="number">0.99</span></span><br><span class="line">    <span class="keyword">if</span> theta[<span class="number">1</span>] &lt; -<span class="number">1</span>:</span><br><span class="line">        theta[<span class="number">1</span>] = -<span class="number">0.99</span></span><br><span class="line"></span><br><span class="line">    log_P = <span class="number">0.5</span> * np.log(<span class="number">1</span> - theta[<span class="number">1</span>] ** <span class="number">2</span>) \</span><br><span class="line">            - <span class="number">0.5</span> * (<span class="number">1</span> - theta[<span class="number">1</span>] ** <span class="number">2</span>) * (x_1 - theta[<span class="number">0</span>] / (<span class="number">1</span> - theta[<span class="number">1</span>])) ** <span class="number">2</span> \</span><br><span class="line">            - <span class="number">0.5</span> * sum_term</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> log_P</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_sum_log_P_y_given_x_and_A</span>(<span class="params">x_series, y_series, theta</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to compute sum_T&#123;log(P(y_t | x_t, A))&#125;</span></span><br><span class="line"><span class="string">    :param x_series: the hidden variable series, x</span></span><br><span class="line"><span class="string">    :param y_series: the observed variable series, y</span></span><br><span class="line"><span class="string">    :param theta: the parameters, where we only need A = theta[2]</span></span><br><span class="line"><span class="string">    :return: the sum over T of log(P(y_t | x_t, A))</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    A_current = theta[<span class="number">2</span>]</span><br><span class="line">    sum_log = -<span class="number">0.5</span> * np.log(<span class="number">2</span> * <span class="number">3.14</span>) * T</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(T):</span><br><span class="line">        sum_log += -<span class="number">0.5</span> * (y_series[i] - x_series[i] * A_current) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> sum_log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accept_or_reject_proposals</span>(<span class="params">x_series, y_series, theta_current, theta_proposal</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to determine the theta_next by accepting or rejecting the proposals</span></span><br><span class="line"><span class="string">    :param x_series: the hidden variable series, x</span></span><br><span class="line"><span class="string">    :param y_series: the observed variable series, y</span></span><br><span class="line"><span class="string">    :param theta_current: the current parameters [\delta_i, \phi_i, A_i]</span></span><br><span class="line"><span class="string">    :param theta_proposal: the proposed parameters [\delta_p, \phi_p, A_p]</span></span><br><span class="line"><span class="string">    :return: the new theta [\delta_&#123;i+1&#125;, \phi_&#123;i+1&#125;, A_&#123;i+1&#125;], the [signals] of accepting (for True) or rejecting (for False)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    accept = [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]</span><br><span class="line">    theta_next = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    log_p_x_theta_i = compute_log_P_x_given_theta(x_series, theta_current)</span><br><span class="line">    log_theta_i = prior_theta(theta_current)</span><br><span class="line">    log_theta_p = prior_theta(theta_proposal)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># + For parameter \delta, to compare log(P(\delta_p | \phi_i, x)) and log(P(\delta_i | \phi_i, x)), which equals to:</span></span><br><span class="line">    <span class="comment"># + log(r) = log(P(x | \delta_p, \phi_i)) + log(P(\delta_p)) - log(P(x | \delta_i, \phi_i)) - log(P(\delta_i))</span></span><br><span class="line">    log_r_delta = compute_log_P_x_given_theta(x_series, [theta_proposal[<span class="number">0</span>], theta_current[<span class="number">1</span>]]) \</span><br><span class="line">                  + log_theta_p[<span class="number">0</span>] \</span><br><span class="line">                  - log_p_x_theta_i \</span><br><span class="line">                  - log_theta_i[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> log_r_delta &gt; <span class="number">0</span>:</span><br><span class="line">        accept[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">        theta_next[<span class="number">0</span>] = theta_proposal[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        log_u = np.log(np.random.uniform(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> log_u &lt; log_r_delta:</span><br><span class="line">            accept[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">            theta_next[<span class="number">0</span>] = theta_proposal[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># ! reject</span></span><br><span class="line">            accept[<span class="number">0</span>] = <span class="literal">False</span></span><br><span class="line">            theta_next[<span class="number">0</span>] = theta_current[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># + For parameter \phi, to compare log(P(\phi_p | \delta_i, x)) and log(P(\phi_i | \delta_i, x)), which equals to:</span></span><br><span class="line">    <span class="comment"># + log(r) = log(P(x | \delta_i, \phi_p)) + log(P(\phi_p)) - log(P(x | \delta_i, \phi_i)) - log(P(\phi_i))</span></span><br><span class="line">    log_r_phi = compute_log_P_x_given_theta(x_series, [theta_current[<span class="number">0</span>], theta_proposal[<span class="number">1</span>]]) \</span><br><span class="line">                + log_theta_p[<span class="number">1</span>] \</span><br><span class="line">                - log_p_x_theta_i \</span><br><span class="line">                - log_theta_i[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> log_r_phi &gt; <span class="number">0</span>:</span><br><span class="line">        accept[<span class="number">1</span>] = <span class="literal">True</span></span><br><span class="line">        theta_next[<span class="number">1</span>] = theta_proposal[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        log_u = np.log(np.random.uniform(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> log_u &lt; log_r_phi:</span><br><span class="line">            accept[<span class="number">1</span>] = <span class="literal">True</span></span><br><span class="line">            theta_next[<span class="number">1</span>] = theta_proposal[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># ! reject</span></span><br><span class="line">            accept[<span class="number">1</span>] = <span class="literal">False</span></span><br><span class="line">            theta_next[<span class="number">1</span>] = theta_current[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># + For parameter A, to compare log(P(A_p | x_series, y_series)) and log(P(A_i | x_series, y_series)), which</span></span><br><span class="line">    <span class="comment"># + equals to: log(r) = sum_T&#123;log(P(y_t | x_t, A_p))&#125; + log(P(A_p)) - sum_T&#123;log(P(y_t | x_t, A_i))&#125; - log(P(A_i))</span></span><br><span class="line">    log_r_A = compute_sum_log_P_y_given_x_and_A(x_series, y_series, theta_proposal) \</span><br><span class="line">              + log_theta_p[<span class="number">2</span>] \</span><br><span class="line">              - compute_sum_log_P_y_given_x_and_A(x_series, y_series, theta_current) \</span><br><span class="line">              - log_theta_i[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> log_r_A &gt; <span class="number">0</span>:</span><br><span class="line">        accept[<span class="number">2</span>] = <span class="literal">True</span></span><br><span class="line">        theta_next[<span class="number">2</span>] = theta_proposal[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        log_u = np.log(np.random.uniform(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> log_u &lt; log_r_phi:</span><br><span class="line">            accept[<span class="number">2</span>] = <span class="literal">True</span></span><br><span class="line">            theta_next[<span class="number">2</span>] = theta_proposal[<span class="number">2</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># ! reject</span></span><br><span class="line">            accept[<span class="number">2</span>] = <span class="literal">False</span></span><br><span class="line">            theta_next[<span class="number">2</span>] = theta_current[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> theta_next, accept</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">perform_mcmc</span>(<span class="params">N, y_series, x_series_0, theta_0</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to perform MCMC sampling.</span></span><br><span class="line"><span class="string">    :param N: total number of samples</span></span><br><span class="line"><span class="string">    :param y_series: the observed variable series, y</span></span><br><span class="line"><span class="string">    :param x_series_0: the starting x_series</span></span><br><span class="line"><span class="string">    :param theta_0: the starting point</span></span><br><span class="line"><span class="string">    :return: the samples</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    samples = [theta_0]</span><br><span class="line">    accept_records = [[<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>]]  <span class="comment"># to record the acceptance</span></span><br><span class="line">    x_series = x_series_0</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1</span>, N)):</span><br><span class="line">        x_series_samples = sample_x_series(y_series, x_series, samples[n - <span class="number">1</span>])</span><br><span class="line">        theta_proposal = generate_next(samples[n - <span class="number">1</span>])</span><br><span class="line">        theta_next, accept = accept_or_reject_proposals(x_series_samples, y_series, samples[n - <span class="number">1</span>], theta_proposal)</span><br><span class="line">        samples.append(theta_next)</span><br><span class="line">        accept_records.append(accept)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> samples, accept_records</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_figures</span>(<span class="params">samples, means</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    The function to plot the samples.</span></span><br><span class="line"><span class="string">    :param samples: the samples</span></span><br><span class="line"><span class="string">    :param means: the means of two sets of samples (without burn-in)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    delta_samples = samples[<span class="number">0</span>]</span><br><span class="line">    phi_samples = samples[<span class="number">1</span>]</span><br><span class="line">    A_samples = samples[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># draw \delta figure</span></span><br><span class="line">    plt.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(N)), delta_samples, label=<span class="string">&quot;samples&quot;</span>)</span><br><span class="line">    plt.vlines(B, <span class="built_in">min</span>(delta_samples), <span class="built_in">max</span>(delta_samples), colors=<span class="string">&quot;r&quot;</span>, label=<span class="string">&quot;burn-in&quot;</span>)</span><br><span class="line">    plt.hlines(means[<span class="number">0</span>], <span class="number">0</span>, N, colors=<span class="string">&quot;m&quot;</span>, label=<span class="string">&quot;mean of samples&quot;</span>)</span><br><span class="line">    plt.hlines(theta_real[<span class="number">0</span>], <span class="number">0</span>, N, colors=<span class="string">&quot;g&quot;</span>, label=<span class="string">&quot;real value&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(<span class="string">r&quot;Samples of \delta&quot;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&quot;./Results/MCMC-HMM-delta.pdf&quot;</span>, bbox_inches=<span class="string">&#x27;tight&#x27;</span>, pad_inches=<span class="number">0</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># draw \phi figure</span></span><br><span class="line">    plt.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(N)), phi_samples, label=<span class="string">&quot;samples&quot;</span>)</span><br><span class="line">    plt.vlines(B, <span class="built_in">min</span>(phi_samples), <span class="built_in">max</span>(phi_samples), colors=<span class="string">&quot;r&quot;</span>, label=<span class="string">&quot;burn-in&quot;</span>)</span><br><span class="line">    plt.hlines(means[<span class="number">1</span>], <span class="number">0</span>, N, colors=<span class="string">&quot;m&quot;</span>, label=<span class="string">&quot;mean of samples&quot;</span>)</span><br><span class="line">    plt.hlines(theta_real[<span class="number">1</span>], <span class="number">0</span>, N, colors=<span class="string">&quot;g&quot;</span>, label=<span class="string">&quot;real value&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(<span class="string">r&quot;Samples of \phi&quot;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&quot;./Results/MCMC-HMM-phi.pdf&quot;</span>, bbox_inches=<span class="string">&#x27;tight&#x27;</span>, pad_inches=<span class="number">0</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># draw A figure</span></span><br><span class="line">    plt.plot(<span class="built_in">list</span>(<span class="built_in">range</span>(N)), A_samples, label=<span class="string">&quot;samples&quot;</span>)</span><br><span class="line">    plt.vlines(B, <span class="built_in">min</span>(A_samples), <span class="built_in">max</span>(A_samples), colors=<span class="string">&quot;r&quot;</span>, label=<span class="string">&quot;burn-in&quot;</span>)</span><br><span class="line">    plt.hlines(means[<span class="number">2</span>], <span class="number">0</span>, N, colors=<span class="string">&quot;m&quot;</span>, label=<span class="string">&quot;mean of samples&quot;</span>)</span><br><span class="line">    plt.hlines(theta_real[<span class="number">2</span>], <span class="number">0</span>, N, colors=<span class="string">&quot;g&quot;</span>, label=<span class="string">&quot;real value&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(<span class="string">r&quot;Samples of A&quot;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&quot;./Results/MCMC-HMM-A.pdf&quot;</span>, bbox_inches=<span class="string">&#x27;tight&#x27;</span>, pad_inches=<span class="number">0</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    y_series = generate_series(T, x_0)</span><br><span class="line">    samples, accept_records = perform_mcmc(N, y_series, x_series_0, theta_0)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># To analyse the results</span></span><br><span class="line">    samples = np.array(samples).T</span><br><span class="line">    samples_without_burn_in = samples[:, B:]</span><br><span class="line">    means = np.mean(samples_without_burn_in, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    plot_figures(samples, means)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Real values of theta (delta, phi, A): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(theta_real))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Estimated values of theta (delta, phi, A): &#123;&#125;&quot;</span>.<span class="built_in">format</span>(means))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Probabilistic Machine Learning</category>
      </categories>
      <tags>
        <tag>Probability Theory</tag>
        <tag>Machine Learning</tag>
        <tag>Math</tag>
      </tags>
  </entry>
</search>
