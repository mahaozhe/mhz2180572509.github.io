<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.9.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust Region Policy Optimization. In Proceedings of the 32nd International Conference on Machine Learning, PM">
<meta property="og:type" content="article">
<meta property="og:title" content="[TRPO] Trust Region Policy Optimization">
<meta property="og:url" content="http://example.com/2022/16-RLPaper-MF-PG-2-TRPO/">
<meta property="og:site_name" content="Life like Wine">
<meta property="og:description" content="John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. Trust Region Policy Optimization. In Proceedings of the 32nd International Conference on Machine Learning, PM">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/RLPapers/Model-Free/PolicyGradients/2-1.png">
<meta property="og:image" content="http://example.com/images/RLPapers/Model-Free/PolicyGradients/2-2.png">
<meta property="og:image" content="http://example.com/images/RLPapers/Model-Free/PolicyGradients/2-3.png">
<meta property="article:published_time" content="2022-11-10T02:48:39.000Z">
<meta property="article:modified_time" content="2022-11-10T03:00:01.811Z">
<meta property="article:author" content="Ma Haozhe">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="Paper Notes">
<meta property="article:tag" content="Model-Free RL">
<meta property="article:tag" content="Policy Gradients">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/RLPapers/Model-Free/PolicyGradients/2-1.png">


<link rel="canonical" href="http://example.com/2022/16-RLPaper-MF-PG-2-TRPO/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2022/16-RLPaper-MF-PG-2-TRPO/","path":"2022/16-RLPaper-MF-PG-2-TRPO/","title":"[TRPO] Trust Region Policy Optimization"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>[TRPO] Trust Region Policy Optimization | Life like Wine</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Life like Wine</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#overview"><span class="nav-number">1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#main-approximation-procedure"><span class="nav-number">2.</span> <span class="nav-text">Main Approximation Procedure</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#preliminaries"><span class="nav-number">2.1.</span> <span class="nav-text">Preliminaries</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mathematically-approximations"><span class="nav-number">2.2.</span> <span class="nav-text">Mathematically
Approximations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#from-conservative-policy-iteration-to-general-stochastic-policies"><span class="nav-number">2.3.</span> <span class="nav-text">From
Conservative Policy Iteration to General Stochastic Policies</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#optimization-of-parameterized-policies"><span class="nav-number">2.4.</span> <span class="nav-text">Optimization of
Parameterized Policies</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sample-based-estimation"><span class="nav-number">2.5.</span> <span class="nav-text">Sample-Based Estimation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#two-variant-sampling-approaches"><span class="nav-number">2.6.</span> <span class="nav-text">Two Variant Sampling
Approaches</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-number">3.</span> <span class="nav-text">Reference</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ma Haozhe"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Ma Haozhe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/mhz2180572509" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;mhz2180572509" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:haozhe.ma@u.nus.edu" title="E-Mail → mailto:haozhe.ma@u.nus.edu" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/16-RLPaper-MF-PG-2-TRPO/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Ma Haozhe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life like Wine">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [TRPO] Trust Region Policy Optimization
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2022-11-10 10:48:39 / Modified: 11:00:01" itemprop="dateCreated datePublished" datetime="2022-11-10T10:48:39+08:00">2022-11-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/RL-Paper-Notes/" itemprop="url" rel="index"><span itemprop="name">RL Paper Notes</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <blockquote>
<p>John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and
Philipp Moritz. 2015. Trust Region Policy Optimization. In Proceedings
of the 32nd International Conference on Machine Learning, PMLR,
1889–1897. Retrieved October 4, 2022 from
https://proceedings.mlr.press/v37/schulman15.html</p>
</blockquote>
<span id="more"></span>
<figure>
<img src="/images/RLPapers/Model-Free/PolicyGradients/2-1.png"
alt="Trust Region Policy Optimization" />
<figcaption aria-hidden="true">Trust Region Policy
Optimization</figcaption>
</figure>
<h1 id="overview">Overview</h1>
<p>The paper proposed a new <strong>practical</strong> algorithm called
<strong><em>Trust Region Policy Optimization</em></strong>
(<strong><em>TRPO</em></strong>). The algorithm is
<strong>practical</strong> as it can be implemented into plenty
large-space environments and continuous control problems. The main
innovation of the TPRO is its derivation based on the
theoretically-justified algorithm [1] to do <strong>many rounds of
reasonable approximations</strong> to make the optimizing objective be
calculated effectively. Most importantly, the paper proves the proposed
algorithm is <strong>guaranteed to be improved monotonically</strong>.
TRPO is a model-free, policy-based, <strong>on-policy</strong>,
<strong>Monte Carlo</strong> algorithm that can be used in environments
with <strong>continuous state space and action space</strong>, and also
support high-dimensional input and large nonlinear policies such as
neural networks. The key idea is how to do approximation of the
optimization objective. The paper conducts experiments in a wide range
of tasks, including the robotic and Atari games domains, which shows
great performance.</p>
<h1 id="main-approximation-procedure">Main Approximation Procedure</h1>
<p>In this section, we will show how does the paper approximate the
optimization objective step-by-step. In the following sub-sections, the
notions used in this paper is a bit different from common ones. To
define the MDP model, it doesn't use the reward function, but uses the
<strong>cost function</strong> <span class="math inline">\(c:S
\rightarrow \mathbb{R}\)</span>, so the main target is to
<strong>minimize the expected cost</strong>, defined as <span
class="math inline">\(\eta(\pi)=\mathbb{E}[\sum_{t=0}^{\infty}{\gamma^t
c(s_t)}]\)</span>.</p>
<h2 id="preliminaries">Preliminaries</h2>
<p>Overall, we know the following basic concepts:</p>
<p><span class="math display">\[\begin{equation}
Q_{\pi}(s_t,a_t)=\mathbb{E}[\sum_{l=0}^{\infty}{\gamma^l c(s_{t+l})}],
a_t \sim \pi(s_t)
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
V_{\pi} (s_t)=\mathbb{E}[\sum_{l=0}^{\infty}{\gamma^l c(s_{t+l})}]
\end{equation}\]</span></p>
<p>We also have the advantage function:</p>
<p><span class="math display">\[\begin{equation}
A_{\pi}(s,a)=Q_{\pi}(s,a)−V_{\pi}(s)
\end{equation}\]</span></p>
<p>Kakade &amp; Lanford (2002) [1] derived the relationship of the
expected discounted cost between two policies <span
class="math inline">\(\pi\)</span> and <span
class="math inline">\(\tilde{\pi}\)</span> to have the following
equation:</p>
<p><span class="math display">\[\begin{equation}
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_0,a_0,s_1,a_1...}[\sum_{t=0}^{\infty}{\gamma^t
A_{\pi}(s_t,a_t)}]
\end{equation}\]</span></p>
<p>Where the transitions are generated based on <strong>the
policy</strong> <span class="math inline">\(\tilde{\pi}\)</span>.</p>
<p>This is very straightforward, the actions are selected from policy
<span class="math inline">\(\tilde{\pi}\)</span> but the advantages are
computed by policy <span class="math inline">\(\pi\)</span>.</p>
<p>The paper introduced the <strong>(unnormalized) discounted visitation
frequencies</strong> as:</p>
<p><span class="math display">\[\begin{equation}
\rho_{\pi}(s)=P(s_0=s)+\gamma P(s_1=s)+...+\gamma^{\tau}
P(s_{\tau}=s)+...
\end{equation}\]</span></p>
<p>So the expected discounted cost can be rearranged as:</p>
<p><span class="math display">\[\begin{equation}
\eta(\tilde{\pi})=\eta(\pi)+\sum_{s}{\rho_{\tilde{\pi}}(s)
\sum_{a}{\tilde{\pi}(a│s) A_{\pi}(s,a)}}\end{equation}\]</span></p>
<p>In this case, we can see that, the cost function for the new policy
<span class="math inline">\(\tilde{\pi}\)</span> equals to adding an
additional term to the original cost function, so if the term <span
class="math inline">\(\sum_{s}{\rho_{\tilde{\pi}}(s)
\sum_{a}{\tilde{\pi}(a│s) A_{\pi}(s,a)}}\)</span> <strong>guarantees to
be a negative value</strong>, then the update can <strong>guarantee to
reduce the cost function</strong> <span
class="math inline">\(\eta\)</span>.</p>
<p>All of the following derivations are based on this formula, and many
rounds of approximations are done from it.</p>
<h2 id="mathematically-approximations">Mathematically
Approximations</h2>
<p>Firstly, the paper defines the <span
class="math inline">\(\eta(\tilde{\pi})\)</span> as the main obejctive
to minimize, denoted as <span
class="math inline">\(L_{\pi}(\tilde{\pi})\)</span>:</p>
<p><span class="math display">\[\begin{equation}
L_{\pi}(\tilde{\pi})=\eta(\pi)+\sum_{s}{\rho_{\tilde{\pi}}(s)
\sum_{a}{\tilde{\pi}(a│s) A_{\pi}(s,a)}}
\end{equation}\]</span></p>
<ul>
<li><p>The <strong><em>first approximation</em></strong>: As the
distribution of the states under a new policy is difficult to get, so it
replaces the new policy <strong>with the old policy</strong>, which
ignores the changes in state visitation density due to changes in the
policy: <span class="math display">\[\begin{equation}
L_{\pi}(\tilde{\pi})=\eta(\pi)+\sum_{s}{\rho_{\pi}(s)
\sum_{a}{\tilde{\pi}(a│s) A_{\pi}(s,a)}}
\end{equation}\]</span> So in this case, we use the old policy <span
class="math inline">\(\pi\)</span> for the state-visitation
distribution.</p></li>
<li><p>Then based on the <strong>conservative policy iteration</strong>:
<span class="math display">\[\begin{equation}
\pi_{new} (a│s)=(1−\alpha)\pi_{old}(a│s)+\alpha \pi&#39;(a|s)
\end{equation}\]</span> And the inequation that Kakade and Langford
proved: <span class="math display">\[\begin{equation}
\eta(\pi_{new}) \leq L_{\pi_{old}}(\pi_{new})+ \frac{2\epsilon
\gamma}{(1−\gamma(1−\alpha))(1−\gamma)\alpha^2}
\end{equation}\]</span> Where <span
class="math inline">\(\epsilon=\max_s{⁡|\mathbb{E}_{a \sim \pi&#39;
(a│s)} [A_{\pi}(s,a)]|}\)</span></p>
<p>The <strong><em>second approximation</em></strong>: As <span
class="math inline">\(\alpha \ll 1\)</span>, we can approximate the
above inequation as:</p>
<p><span class="math display">\[\begin{equation}
\eta(\pi_{new}) \leq L_{\pi_{old}}(\pi_{new})+\frac{2 \epsilon
\gamma}{(1−\gamma)^2 \alpha^2}
\end{equation}\]</span></p></li>
</ul>
<h2
id="from-conservative-policy-iteration-to-general-stochastic-policies">From
Conservative Policy Iteration to General Stochastic Policies</h2>
<p>The second approximation is based on the conservative policy
iteration, however, which is not commonly used in a wide range of
problems, so make the algorithm applicable to more general stochastic
policies methods, the crucial is to eliminate the <strong>step
factor</strong> to extend the ineuqation (the guarantee) to practical
problems.</p>
<ul>
<li>The <strong><em>third approximation</em></strong>: replace <span
class="math inline">\(\alpha\)</span> with a distance measure between
<span class="math inline">\(\pi\)</span> and <span
class="math inline">\(\tilde{\pi}\)</span>. The paper uses the
<strong>total variance divergence</strong> to replace <span
class="math inline">\(\alpha\)</span>: <span
class="math display">\[\begin{equation}
\alpha=D_{TE}^{max}(\pi_{old},\pi_{new})
\end{equation}\]</span></li>
<li>The <strong><em>fourth approximation</em></strong>: replace the
total variance divergence with the <strong>KL divergence</strong> based
on their unequal relationship <span class="math inline">\(D_{TV}(p||q)^2
\leq D_{K}(p||q)\)</span>: <span class="math display">\[\begin{equation}
\alpha=D_{KL}^{max}(\pi_{old},\pi_{new})
\end{equation}\]</span></li>
</ul>
<p>For now, based on the finally derived objective to be minimized, the
algorithm can be shown as</p>
<figure>
<img src="/images/RLPapers/Model-Free/PolicyGradients/2-2.png"
alt="Algorithm: Approximate policy iteration algorithm guaranteeing non-increasing expected cost" />
<figcaption aria-hidden="true">Algorithm: Approximate policy iteration
algorithm guaranteeing non-increasing expected cost</figcaption>
</figure>
<p>The algorithm provides the theorical formula to update the policies
based on the assumption that we know all of the transition functions and
we can directly compute the <span class="math inline">\(argmin\)</span>
part. However, we usually have no exact form of the policies, which
means we <strong>cannot directly compute the</strong> <span
class="math inline">\(argmin\)</span>, so in the following sections, we
consider the <strong>parameterized</strong> policy function <span
class="math inline">\(\pi_{\theta}\)</span>.</p>
<h2 id="optimization-of-parameterized-policies">Optimization of
Parameterized Policies</h2>
<p>In this section, we consider the <strong>parameterized</strong>
policy function <span class="math inline">\(\pi_{\theta}\)</span> to see
how can we directly update the parameters from <span
class="math inline">\(\theta_{old}\)</span> to <span
class="math inline">\(\theta\)</span>. To rewrite the objective in a
parameterized form, we have:</p>
<p><span class="math inline">\(\eta(\theta) &lt;
L_{\theta_{old}}(\theta)+C
D_{KL}^{max}(\theta_{old},\theta)\)</span></p>
<ul>
<li>The <strong><em>fifth approximation</em></strong>: to use a
constraint on the KL divergence between the new policy and the old
policy, i.e., a <strong>trust region constraint</strong>: <span
class="math display">\[\begin{equation}
\min_{\theta}[L_{\theta_{old}}(\theta)]
\end{equation}\]</span> <span class="math display">\[\begin{equation}
s.t. D_{KL}^{max}(\theta_{old},\theta)\leq \delta
\end{equation}\]</span></li>
<li>The <strong><em>sixth approximation</em></strong>: as the <span
class="math inline">\(max\)</span> operator makes the optimizaion hard,
so we can use the average KL divergence instead of the maximal: <span
class="math display">\[\begin{equation}
\min_{\theta}[L_{\theta_{old}}(\theta)]
\end{equation}\]</span> <span class="math display">\[\begin{equation}
s.t. D_{KL}^{\rho_{\theta_{old}}}(\theta_{old},\theta) \leq \delta
\end{equation}\]</span></li>
</ul>
<h2 id="sample-based-estimation">Sample-Based Estimation</h2>
<p>Now we have had the theoretical formula, but in practice, we cannot
compute the exact value of the main objective and the constraint, we
usually need to estimate these values by sampling, so in this section,
the paper introduced how to use <strong><em>importance
sampling</em></strong> to estimate the two values.</p>
<ul>
<li>The <strong><em>seventh approximation</em></strong>: to use
importance sampling to estimate the expectation term: <span
class="math display">\[\begin{equation}
\sum_{a}{\pi_{\theta}(a│s_n)A_{\pi_{\theta}}(s_n,a)} \approx
\mathbb{E}_{a \sim q} \Big[\frac{\pi_{\theta}(a│s)}{q(a│s)}
A_{\theta_{old}}(s,a)\Big]
\end{equation}\]</span></li>
<li>The <strong><em>eighth approximation</em></strong>: to avoid
computing the advantage functions, just replace it using the
Q-functions, so the final objective to optimize is: <span
class="math display">\[\begin{equation}
\min_{\theta}⁡{\mathbb{E}_{s \sim \rho_{\theta_{old}}, a \sim
q}\Big[\frac{\pi_{\theta}(a│s)}{q(a│s)} A_{\theta_{old}}(s,a)\Big]}
\end{equation}\]</span> <span class="math display">\[\begin{equation}
s.t. \mathbb{E}_{s \sim \rho_{\theta_{old}}} [D_{KL}
(\pi_{\theta_{old}}(\cdot│s)||\pi_{\theta} (\cdot|s))] \leq \delta
\end{equation}\]</span></li>
</ul>
<h2 id="two-variant-sampling-approaches">Two Variant Sampling
Approaches</h2>
<p>The paper further proposed two variant sampling approaches:</p>
<ul>
<li><strong>Single Path</strong>: typically used for policy estimation
and is based on sampling individual trajectories.</li>
<li><strong>Vine</strong>: construct a rollout set and then perform
multiple actions from each sate in the rollout set.</li>
</ul>
<figure>
<img src="/images/RLPapers/Model-Free/PolicyGradients/2-3.png"
alt="Two variant sampling approaches" />
<figcaption aria-hidden="true">Two variant sampling
approaches</figcaption>
</figure>
<h1 id="reference">Reference</h1>
<p>[1] Sham Kakade and John Langford. 2002. Approximately Optimal
Approximate Reinforcement Learning. In Proceedings of the Nineteenth
International Conference on Machine Learning (ICML ’02), Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA, 267–274.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/Paper-Notes/" rel="tag"># Paper Notes</a>
              <a href="/tags/Model-Free-RL/" rel="tag"># Model-Free RL</a>
              <a href="/tags/Policy-Gradients/" rel="tag"># Policy Gradients</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/15-RLPaper-MF-PG-1-A3C/" rel="prev" title="[A3C] Asynchronous Methods for Deep Reinforcement Learning">
                  <i class="fa fa-chevron-left"></i> [A3C] Asynchronous Methods for Deep Reinforcement Learning
                </a>
            </div>
            <div class="post-nav-item">
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ma Haozhe</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
